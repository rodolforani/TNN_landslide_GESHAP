{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.chdir(\"/home/dahala/Transformer-Gorkha/\")\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from src import preparedata\n",
    "from src import transformermodel\n",
    "from src import traintransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put inference true\n",
    "params = json.load(open(\"params/params.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#landslidehazard.model.load_weights(\"checkpoints/DS_daily_75.h5\")\n",
    "model = tf.keras.models.load_model(\"checkpoints/DS_daily_75.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#landslidehazard.model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Xt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention weights and Grad_AAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'multi_head_attention' is the name of your attention layer\n",
    "attention_layer = model.get_layer(\"multi_head_attention\")\n",
    "\n",
    "# Inputs to the model\n",
    "input_Xt = dataset.Xt  # Temporal input\n",
    "input_Xc = dataset.Xc  # Static input\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass through the attention layer\n",
    "    query = input_Xt\n",
    "    key = input_Xt\n",
    "    value = input_Xt\n",
    "    _, attention_weights = attention_layer(\n",
    "        query=query, key=key, value=value, return_attention_scores=True\n",
    "    )\n",
    "\n",
    "    # Watch attention weights\n",
    "    tape.watch(attention_weights)\n",
    "\n",
    "    # Forward pass through the full model\n",
    "    prediction = model([input_Xt, input_Xc])\n",
    "\n",
    "# Compute gradients of the prediction with respect to attention weights\n",
    "gradients = tape.gradient(prediction, attention_weights)\n",
    "\n",
    "# Check if gradients are None\n",
    "if gradients is None:\n",
    "    raise ValueError(\"Gradients could not be computed. Check if attention_weights are connected to the prediction.\")\n",
    "\n",
    "# Average attention weights across heads\n",
    "avg_attention_weights = tf.reduce_mean(attention_weights, axis=1)  # Shape: (69159, 31, 31)\n",
    "\n",
    "# Combine gradients with attention weights to calculate Grad-AAM scores\n",
    "grad_aam_scores = avg_attention_weights * gradients\n",
    "\n",
    "# Average Grad-AAM scores across heads\n",
    "avg_grad_aam_scores = tf.reduce_mean(grad_aam_scores, axis=1).numpy()\n",
    "\n",
    "# Plot the Grad-AAM scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(avg_grad_aam_scores[0], cmap=\"viridis\", aspect=\"auto\")  # Use 0th sample\n",
    "plt.colorbar(label=\"Grad-AAM Score\")\n",
    "plt.title(\"Grad-AAM Heatmap\")\n",
    "plt.xlabel(\"Key Timestep\")\n",
    "plt.ylabel(\"Query Timestep\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores from integrated gradients (IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_features = dataset.Xt.shape[2]\n",
    "num_instances = dataset.Xt.shape[0]\n",
    "all_ig_scores = []\n",
    "all_sensitivity_scores = []\n",
    "\n",
    "batch_size = 200\n",
    "batches = int(np.ceil(num_instances / batch_size))  # Ensure we process all instances\n",
    "num_steps = 50  # Number of steps for the IG approximation\n",
    "\n",
    "for i in tqdm(range(batches)):\n",
    "    start = batch_size * i\n",
    "    end = min(batch_size * (i + 1), num_instances)  # Ensure we do not exceed the array bounds\n",
    "    dyn = dataset.Xt[start:end, :, :]\n",
    "    stt = np.nan_to_num(dataset.Xc[start:end], 0)  # Ensure we use all features in stt\n",
    "\n",
    "    # Baseline input (all zeros)\n",
    "    bb = np.zeros_like(dyn)\n",
    "    stt_tensor = tf.convert_to_tensor(stt, dtype=tf.float32)  # Convert stt to tensor\n",
    "\n",
    "    # Convert inputs to tensors\n",
    "    dyn_tensor = tf.convert_to_tensor(dyn, dtype=tf.float32)\n",
    "    bb_tensor = tf.convert_to_tensor(bb, dtype=tf.float32)\n",
    "\n",
    "    # Calculate Integrated Gradients and Sensitivity\n",
    "    total_gradients = np.zeros_like(dyn)\n",
    "    \n",
    "    for alpha in np.linspace(0, 1, num_steps):\n",
    "        # Interpolate between the baseline and the input\n",
    "        interpolated_input = bb_tensor + alpha * (dyn_tensor - bb_tensor)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated_input)\n",
    "            output = model([interpolated_input, stt_tensor])\n",
    "            gradients = tape.gradient(output, interpolated_input)\n",
    "        \n",
    "        # Accumulate gradients for each step\n",
    "        total_gradients += gradients.numpy()\n",
    "\n",
    "    # Calculate average sensitivity (mean gradient over steps)\n",
    "    average_sensitivity = total_gradients / num_steps\n",
    "    \n",
    "    # Calculate Integrated Gradients for each day (non-cumulative)\n",
    "    integrated_gradients = (dyn - bb) * average_sensitivity\n",
    "    \n",
    "    # Append both results to separate lists\n",
    "    all_ig_scores.append(integrated_gradients)\n",
    "    all_sensitivity_scores.append(average_sensitivity)\n",
    "\n",
    "# Concatenate all scores to form single arrays\n",
    "all_ig_scores = np.concatenate(all_ig_scores, axis=0)\n",
    "all_sensitivity_scores = np.concatenate(all_sensitivity_scores, axis=0)\n",
    "\n",
    "# Calculate cumulative IG scores\n",
    "#cumulative_ig_scores = np.cumsum(all_ig_scores, axis=0)\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of dataset.Xt: {dataset.Xt.shape}\")\n",
    "print(f\"Shape of all_ig_scores: {all_ig_scores.shape}\")\n",
    "#print(f\"Shape of cumulative_ig_scores: {cumulative_ig_scores.shape}\")\n",
    "print(f\"Shape of all_sensitivity_scores: {all_sensitivity_scores.shape}\")\n",
    "\n",
    "# Save the computed scores if the shapes match\n",
    "if all_ig_scores.shape == dataset.Xt.shape and all_sensitivity_scores.shape == dataset.Xt.shape:\n",
    "    np.save(\"ExplainedGradients/all_scores_RS_daily_75_IG.npy\", all_ig_scores)  # IG scores per day\n",
    "    np.save(\"ExplainedGradients/all_scores_RS_daily_75_IG_sensitivity.npy\", all_sensitivity_scores)  # Sensitivity scores\n",
    "    #np.save(\"ExplainedGradients/all_scores_DF_daily_75_cumulative_IG.npy\", cumulative_ig_scores)  # Cumulative IG scores\n",
    "else:\n",
    "    print(\"Mismatch in total number of elements. Check the computation of the scores.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the values of the scores in all_scores for row 52032\n",
    "row_index = 52032\n",
    "\n",
    "# Check if row_index is within the bounds of all_scores\n",
    "if row_index < all_ig_scores.shape[0]:\n",
    "    # Print the scores for the specified row\n",
    "    print(all_ig_scores[row_index])\n",
    "else:\n",
    "    print(f\"Row index {row_index} is out of bounds. The total number of rows is {all_ig_scores.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IG computation with cumulative dyn and cum-1 baseline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Assuming dataset.Xt is structured as (num_instances, num_days, num_features)\n",
    "num_features = dataset.Xt.shape[2]\n",
    "num_instances = dataset.Xt.shape[0]\n",
    "num_days = dataset.Xt.shape[1]\n",
    "\n",
    "all_ig_scores = []\n",
    "\n",
    "batch_size = 200\n",
    "batches = int(np.ceil(num_instances / batch_size))\n",
    "num_steps = 50  # Number of steps for the IG approximation\n",
    "\n",
    "for i in tqdm(range(batches)):\n",
    "    start = batch_size * i\n",
    "    end = min(batch_size * (i + 1), num_instances)\n",
    "    dyn = dataset.Xt[start:end, :, :]\n",
    "    stt = np.nan_to_num(dataset.Xc[start:end], 0)\n",
    "\n",
    "    # Compute cumulative values for `dyn` to create `dyn_cumulative`\n",
    "    dyn_cumulative = np.cumsum(dyn, axis=1)\n",
    "    \n",
    "    # Create the baseline `bb` by shifting cumulative values by one day\n",
    "    baseline_cumulative = np.zeros_like(dyn_cumulative)\n",
    "    #baseline_cumulative[:, 1:, :] = dyn_cumulative[:, :-1, :]\n",
    "    \n",
    "    # Convert `dyn_cumulative` and `baseline_cumulative` to tensors\n",
    "    dyn_tensor = tf.convert_to_tensor(dyn_cumulative, dtype=tf.float32)\n",
    "    bb_tensor = tf.convert_to_tensor(baseline_cumulative, dtype=tf.float32)\n",
    "    stt_tensor = tf.convert_to_tensor(stt, dtype=tf.float32)\n",
    "\n",
    "    # Initialize gradient accumulation\n",
    "    total_gradients = np.zeros_like(dyn_cumulative)\n",
    "\n",
    "    for alpha in np.linspace(0, 1, num_steps):\n",
    "        # Interpolate between the baseline and cumulative input values\n",
    "        interpolated_input = bb_tensor + alpha * (dyn_tensor - bb_tensor)\n",
    "        print(f\"Alpha: {alpha}, Interpolated Input Sample:\", interpolated_input[0, :5].numpy())  # Sample output for debug\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated_input)\n",
    "            output = model([interpolated_input, stt_tensor])\n",
    "            print(f\"Output Sample (Alpha={alpha}):\", output.numpy()[0])  # Sample output for debug\n",
    "            gradients = tape.gradient(output, interpolated_input)\n",
    "        \n",
    "        print(\"Gradient Sample:\", gradients[0, :5].numpy())  # Sample gradients for debug\n",
    "        \n",
    "        # Accumulate gradients for each step\n",
    "        total_gradients += gradients.numpy()\n",
    "\n",
    "    # Calculate the average gradient and Integrated Gradients for each day\n",
    "    average_sensitivity = total_gradients / num_steps\n",
    "    integrated_gradients = (dyn_cumulative - baseline_cumulative) * average_sensitivity\n",
    "\n",
    "    # Append the IG scores for the batch\n",
    "    all_ig_scores.append(integrated_gradients)\n",
    "\n",
    "# Concatenate all IG scores into a single array\n",
    "all_ig_scores = np.concatenate(all_ig_scores, axis=0)\n",
    "\n",
    "# Verify the shape\n",
    "print(f\"Shape of dataset.Xt: {dataset.Xt.shape}\")\n",
    "print(f\"Shape of all_ig_scores: {all_ig_scores.shape}\")\n",
    "\n",
    "# Save the computed IG scores\n",
    "if all_ig_scores.shape == dataset.Xt.shape:\n",
    "    np.save(\"ExplainedGradients/all_scores_DS_daily_75_IG_cum.npy\", all_ig_scores)\n",
    "else:\n",
    "    print(\"Mismatch in total number of elements. Check the computation of IG scores.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradients to obtain the SHAP equivalent (old original proposed method - not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_features = dataset.Xt.shape[2]\n",
    "num_instances = dataset.Xt.shape[0]\n",
    "all_scores = []\n",
    "\n",
    "batch_size = 200\n",
    "batches = int(np.ceil(num_instances / batch_size))  # Ensure we process all instances\n",
    "\n",
    "for i in tqdm(range(batches)):\n",
    "    start = batch_size * i\n",
    "    end = min(batch_size * (i + 1), num_instances)  # Ensure we do not exceed the array bounds\n",
    "    dyn = dataset.Xt[start:end, :, :]\n",
    "    stt = np.nan_to_num(dataset.Xc[start:end], 0)  # Ensure we use all features in stt\n",
    "    \n",
    "    bb = tf.convert_to_tensor(np.zeros(dyn.shape), dtype=tf.float32)\n",
    "    stt_tensor = tf.convert_to_tensor(stt, dtype=tf.float32)  # Convert stt to a tensor with correct shape # turn it 0 to evaluate the influence of the static preds and put the bb as non zer = dyn\n",
    "     \n",
    "    with tf.GradientTape() as tape_baseline:\n",
    "        tape_baseline.watch(bb)\n",
    "        output_baseline = model([bb, stt_tensor])  # Use the tensor version of stt\n",
    "        gradients_baseline = tape_baseline.gradient(output_baseline, bb)\n",
    "    \n",
    "    xx = tf.convert_to_tensor(dyn, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape_instance:\n",
    "        tape_instance.watch(xx)\n",
    "        output = model([xx, stt_tensor])  # Use the tensor version of stt\n",
    "        gradients_instances = tape_instance.gradient(output, xx)\n",
    "    \n",
    "    scoresv2 = (xx - bb) * gradients_instances / (gradients_instances - gradients_baseline + 1e-10)\n",
    "    del gradients_instances, gradients_baseline\n",
    "    all_scores.append(scoresv2.numpy())\n",
    "    del scoresv2\n",
    "\n",
    "# Concatenate all scores to form a single array\n",
    "all_scores = np.concatenate(all_scores, axis=0)\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of dataset.Xt: {dataset.Xt.shape}\")\n",
    "print(f\"Shape of all_scores: {all_scores.shape}\")\n",
    "print(f\"Total elements in dataset.Xt: {np.prod(dataset.Xt.shape)}\")\n",
    "print(f\"Total elements in all_scores: {np.prod(all_scores.shape)}\")\n",
    "\n",
    "# Reshape and save the computed scores if the shapes match\n",
    "if all_scores.shape == dataset.Xt.shape:\n",
    "    all_scores_3d = all_scores.reshape(dataset.Xt.shape)\n",
    "    np.save(f\"ExplainedGradients/all_scores_RS_hourly.npy\", all_scores_3d)\n",
    "else:\n",
    "    print(\"Mismatch in total number of elements. Check the computation of all_scores.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient from SHAP equivalent for static predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Number of static features\n",
    "num_static_features = dataset.Xc.shape[1]\n",
    "num_instances = dataset.Xt.shape[0]\n",
    "all_scores_static = []\n",
    "\n",
    "batch_size = 200\n",
    "batches = int(np.ceil(num_instances / batch_size))  # Ensure we process all instances\n",
    "\n",
    "for i in tqdm(range(batches)):\n",
    "    start = batch_size * i\n",
    "    end = min(batch_size * (i + 1), num_instances)  # Ensure we do not exceed the array bounds\n",
    "    \n",
    "    # Dynamic features (time series) - keep unchanged\n",
    "    dyn = dataset.Xt[start:end, :, :]\n",
    "    \n",
    "    # Static features - original and baseline (all zeros tensor)\n",
    "    stt_original = np.nan_to_num(dataset.Xc[start:end], 0)  # Ensure we use all features in stt\n",
    "    stt_baseline = tf.convert_to_tensor(np.zeros(stt_original.shape), dtype=tf.float32)  # Set to zeros as a baseline\n",
    "    \n",
    "    dyn_tensor = tf.convert_to_tensor(dyn, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape_baseline:\n",
    "        tape_baseline.watch(stt_baseline)\n",
    "        output_baseline = model([dyn_tensor, stt_baseline])  # Use the baseline static context\n",
    "        gradients_baseline = tape_baseline.gradient(output_baseline, stt_baseline)\n",
    "    \n",
    "    stt_tensor = tf.convert_to_tensor(stt_original, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape_instance:\n",
    "        tape_instance.watch(stt_tensor)\n",
    "        output = model([dyn_tensor, stt_tensor])  # Use the original static context\n",
    "        gradients_instances = tape_instance.gradient(output, stt_tensor)\n",
    "    \n",
    "    scores_static = (stt_tensor - stt_baseline) * gradients_instances / (gradients_instances - gradients_baseline + 1e-10)\n",
    "    del gradients_instances, gradients_baseline\n",
    "    all_scores_static.append(scores_static.numpy())\n",
    "    del scores_static\n",
    "\n",
    "# Concatenate all scores to form a single array\n",
    "all_scores_static = np.concatenate(all_scores_static, axis=0)\n",
    "\n",
    "# Verify the shapes again\n",
    "print(f\"Shape of dataset.Xc: {dataset.Xc.shape}\")\n",
    "print(f\"Shape of all_scores_static: {all_scores_static.shape}\")\n",
    "print(f\"Total elements in dataset.Xc: {np.prod(dataset.Xc.shape)}\")\n",
    "print(f\"Total elements in all_scores_static: {np.prod(all_scores_static.shape)}\")\n",
    "\n",
    "# Reshape and save the computed scores if the shapes match\n",
    "if all_scores_static.shape == dataset.Xc.shape:\n",
    "    np.save(f\"ExplainedGradients/all_scores_static_RS_daily_75.npy\", all_scores_static)\n",
    "else:\n",
    "    print(\"Mismatch in total number of elements. Check the computation of all_scores_static.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP GE (new method using python function - ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Combine inputs into a list\n",
    "inputs = [dataset.Xt, dataset.Xc]\n",
    "\n",
    "# base inputs\n",
    "\n",
    "# Create SHAP explainer\n",
    "#explainer = shap.GradientExplainer(model, inputs)\n",
    "# Compute SHAP values\n",
    "#shap_values = explainer.shap_values(inputs)\n",
    "\n",
    "# Define the explainer with optimized parameters for your setup\n",
    "explainer = shap.GradientExplainer(\n",
    "    model,\n",
    "    inputs,\n",
    "    batch_size=150,          # Leverage good computational resources\n",
    "    local_smoothing=0.1    # Slightly increase smoothing for better stability\n",
    ")\n",
    "\n",
    "# Compute SHAP values with adjusted settings, including variance output\n",
    "shap_values, shap_variances = explainer.shap_values(\n",
    "    inputs,\n",
    "    nsamples=200,            # Increase samples slightly for better accuracy\n",
    "    ranked_outputs=None,     # Explain all outputs\n",
    "    return_variances=True    # Include uncertainty estimates\n",
    ")\n",
    "\n",
    "# Check shapes and lengths of SHAP values\n",
    "print(\"Shape of SHAP values for Xt (time series):\", shap_values[0].shape)\n",
    "print(\"Shape of SHAP values for Xc (constant terrain):\", shap_values[1].shape)\n",
    "#print(\"Shape of SHAP variances for Xt (time series):\", shap_variances[0].shape)\n",
    "#print(\"Shape of SHAP variances for Xc (constant terrain):\", shap_variances[1].shape)\n",
    "\n",
    "# Save SHAP values and variances for each input separately\n",
    "np.save(\"ExplainedGradients/DS_shap_ge_values_Xt.npy\", shap_values[0])  # SHAP values for Xt\n",
    "np.save(\"ExplainedGradients/DS_shap_ge_values_Xc.npy\", shap_values[1])  # SHAP values for Xc\n",
    "#np.save(\"ExplainedGradients/DS_shap_ge_variances_Xt.npy\", shap_variances[0])  # Variances for Xt\n",
    "#np.save(\"ExplainedGradients/DS_shap_ge_variances_Xc.npy\", shap_variances[1])  # Variances for Xc\n",
    "\n",
    "print(\"SHAP values saved successfully for Xt and Xc.\")\n",
    "\n",
    "# Squeeze SHAP values for Xt (time series) and Xc\n",
    "shap_values_Xt = np.squeeze(shap_values[0])  # Shape: (69159, 31) after squeezing\n",
    "shap_values_Xc = np.squeeze(shap_values[1])  # Shape: (69159, 32)\n",
    "\n",
    "# Compute mean, 10th percentile, and 90th percentile across all rows for each column (Xt)\n",
    "mean_shap = np.mean(shap_values_Xt, axis=0)  # Mean for each day\n",
    "p10_shap = np.percentile(shap_values_Xt, 10, axis=0)  # 10th percentile for each day\n",
    "p90_shap = np.percentile(shap_values_Xt, 90, axis=0)  # 90th percentile for each day\n",
    "\n",
    "# Plot SHAP values for Xt (time series)\n",
    "days = np.arange(1, shap_values_Xt.shape[1] + 1)  # Day indices (1 to 31)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(days, mean_shap, label=\"Mean SHAP Value\", color=\"blue\", linewidth=2)\n",
    "plt.fill_between(days, p10_shap, p90_shap, color=\"blue\", alpha=0.3, label=\"10-90 Percentile\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Mean SHAP Values with 10th and 90th Percentiles Over Time (Xt)\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"SHAP Values\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the first plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot SHAP values for Xc (static predictors) as a box plot (without outliers)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(\n",
    "    shap_values_Xc,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    whiskerprops=dict(color=\"black\"),\n",
    "    boxprops=dict(color=\"blue\", linewidth=2),\n",
    "    capprops=dict(color=\"black\"),\n",
    "    flierprops=dict(marker=\".\", markersize=0),  # Remove outliers by making them invisible\n",
    "    showfliers=False,  # Explicitly turn off outliers\n",
    ")\n",
    "\n",
    "# Customize the box plot\n",
    "plt.title(\"SHAP Values for Static Predictors\")\n",
    "plt.xlabel(\"Static Predictors\")\n",
    "plt.ylabel(\"SHAP Values\")\n",
    "plt.xticks(\n",
    "    np.arange(1, shap_values_Xc.shape[1] + 1),\n",
    "    [f\"Feature {i}\" for i in range(1, shap_values_Xc.shape[1] + 1)],\n",
    "    rotation=90,\n",
    ")\n",
    "\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the second plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH DEEP EXPLAINER\n",
    "\n",
    "# The error occurs because TensorFlow cannot compute gradients for custom or unsupported operations (shap_LeakyRelu in this case) in your model when using the DeepExplainer.\n",
    "\n",
    "# Why This Happens:\n",
    "# DeepExplainer relies on the TensorFlow backend to compute gradients for all operations in your model. If your model contains unsupported operations (e.g., a custom activation function or operation), TensorFlow raises this LookupError.\n",
    "\n",
    "# Possible Solutions:\n",
    "# Use GradientExplainer Instead: Since GradientExplainer uses numerical approximation and doesn't require the gradients to be directly computable, it might avoid this issue. If you don't need DeepExplainer specifically, revert to GradientExplainer.\n",
    "\n",
    "# python:\n",
    "# explainer = shap.GradientExplainer(model, inputs)\n",
    "# shap_values = explainer.shap_values(inputs)\n",
    "\n",
    "# Replace Unsupported Layers/Activations: If you want to use DeepExplainer, ensure all custom operations are replaced with TensorFlow-supported alternatives. For example:\n",
    "# Replace custom shap_LeakyRelu with TensorFlow's tf.keras.layers.LeakyReLU.\n",
    "    \n",
    "# python:\n",
    "# from tensorflow.keras.layers import LeakyReLU\n",
    "# # Replace shap_LeakyRelu in your model definition with LeakyReLU\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "# Custom Gradient for Unsupported Operations: If you cannot replace the unsupported operations, you can define custom gradients for those operations. However, this is more advanced and requires modifying the TensorFlow backend.\n",
    "# Use KernelExplainer or Other SHAP Explainers: If your input data is not large and you can precompute a subset of background data, you can use KernelExplainer, which doesn't rely on gradients:\n",
    "\n",
    "# python:\n",
    "# explainer = shap.KernelExplainer(model.predict, background_data)\n",
    "# shap_values = explainer.shap_values(inputs)\n",
    "\n",
    "# Check TensorFlow Version Compatibility: Ensure that the TensorFlow version is compatible with SHAP. If you are using an older or newer version of TensorFlow, consider downgrading/upgrading.\n",
    "# Recommended Path:\n",
    "# Given the issue with unsupported operations (shap_LeakyRelu), the most straightforward solution is to switch to GradientExplainer or replace unsupported operations with standard TensorFlow operations if you need DeepExplainer\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine inputs into a list\n",
    "inputs = [dataset.Xt, dataset.Xc]\n",
    "\n",
    "# Create SHAP explainer using DeepExplainer\n",
    "explainer = shap.DeepExplainer(model, inputs)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(inputs)\n",
    "\n",
    "# Check shapes and lengths of SHAP values\n",
    "print(\"Shape of SHAP values for Xt (time series):\", shap_values[0].shape)\n",
    "print(\"Shape of SHAP values for Xc (constant terrain):\", shap_values[1].shape)\n",
    "\n",
    "# Save SHAP values for each input separately\n",
    "np.save(\"ExplainedGradients/DS_shap_DE_values_Xt.npy\", shap_values[0])  # SHAP values for Xt\n",
    "np.save(\"ExplainedGradients/DS_shap_DE_values_Xc.npy\", shap_values[1])  # SHAP values for Xc\n",
    "\n",
    "print(\"SHAP values saved successfully for Xt and Xc.\")\n",
    "\n",
    "# Squeeze SHAP values for Xt (time series) and Xc\n",
    "shap_values_Xt = np.squeeze(shap_values[0])  # Shape: (69159, 31) after squeezing\n",
    "shap_values_Xc = np.squeeze(shap_values[1])  # Shape: (69159, 32)\n",
    "\n",
    "# Compute mean, 10th percentile, and 90th percentile across all rows for each column (Xt)\n",
    "mean_shap = np.mean(shap_values_Xt, axis=0)  # Mean for each day\n",
    "p10_shap = np.percentile(shap_values_Xt, 10, axis=0)  # 10th percentile for each day\n",
    "p90_shap = np.percentile(shap_values_Xt, 90, axis=0)  # 90th percentile for each day\n",
    "\n",
    "# Plot SHAP values for Xt (time series)\n",
    "days = np.arange(1, shap_values_Xt.shape[1] + 1)  # Day indices (1 to 31)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(days, mean_shap, label=\"Mean SHAP Value\", color=\"blue\", linewidth=2)\n",
    "plt.fill_between(days, p10_shap, p90_shap, color=\"blue\", alpha=0.3, label=\"10-90 Percentile\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Mean SHAP Values with 10th and 90th Percentiles Over Time (Xt)\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"SHAP Values\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the first plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot SHAP values for Xc (static predictors) as a box plot (without outliers)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(\n",
    "    shap_values_Xc,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    whiskerprops=dict(color=\"black\"),\n",
    "    boxprops=dict(color=\"blue\", linewidth=2),\n",
    "    capprops=dict(color=\"black\"),\n",
    "    flierprops=dict(marker=\".\", markersize=0),  # Remove outliers by making them invisible\n",
    "    showfliers=False,  # Explicitly turn off outliers\n",
    ")\n",
    "\n",
    "# Customize the box plot\n",
    "plt.title(\"SHAP Values for Static Predictors\")\n",
    "plt.xlabel(\"Static Predictors\")\n",
    "plt.ylabel(\"SHAP Values\")\n",
    "plt.xticks(\n",
    "    np.arange(1, shap_values_Xc.shape[1] + 1),\n",
    "    [f\"Feature {i}\" for i in range(1, shap_values_Xc.shape[1] + 1)],\n",
    "    rotation=90,\n",
    ")\n",
    "\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the second plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twente_old_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
