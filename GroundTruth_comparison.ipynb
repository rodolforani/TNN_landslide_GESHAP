{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to presence/absence map of TP/FP/TN/FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize the resulting shapefile matching with hillshade 15m\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the base paths\n",
    "base_path = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Results_GIS/\"\n",
    "reference_raster_path = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Hillshade_15m_clip.tif\"\n",
    "\n",
    "# Load the reference raster\n",
    "with rasterio.open(reference_raster_path) as ref_raster:\n",
    "    ref_transform = ref_raster.transform\n",
    "    ref_width = ref_raster.width\n",
    "    ref_height = ref_raster.height\n",
    "    ref_crs = ref_raster.crs\n",
    "\n",
    "# List of shapefiles\n",
    "shapefiles = [\n",
    "    f\"{base_path}allProbs_hourly.shp\", \n",
    "    # f\"{base_path}allProbs_daily_75.shp\", \n",
    "    # f\"{base_path}allProbs_terrain_allCum.shp\", \n",
    "    # f\"{base_path}allProbs_terrain_3wCum.shp\",\n",
    "    # f\"{base_path}allProbs_terrain_2wCum.shp\", \n",
    "    # f\"{base_path}allProbs_terrain_1wCum.shp\", \n",
    "    # f\"{base_path}allProbs_terrain_MonCum.shp\", \n",
    "    # f\"{base_path}allProbs_terrain.shp\"\n",
    "]\n",
    "\n",
    "# Output directory for rasters\n",
    "output_dir = f\"{base_path}rasters/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def shapefile_to_raster_with_snap(shapefile, field, output_raster):\n",
    "    # Load the shapefile\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    # Rasterize the geometry, snapping to the reference grid\n",
    "    shapes = ((geom, value) for geom, value in zip(gdf.geometry, gdf[field]))\n",
    "    rasterized = rasterize(\n",
    "        shapes=shapes, \n",
    "        out_shape=(ref_height, ref_width),\n",
    "        transform=ref_transform,\n",
    "        fill=2,  # NoData value set to 2\n",
    "        dtype=rasterio.float32\n",
    "    )\n",
    "    \n",
    "    # Save the raster to a file with the same grid as the reference raster\n",
    "    with rasterio.open(\n",
    "        output_raster, 'w',\n",
    "        driver='GTiff',\n",
    "        height=ref_height,\n",
    "        width=ref_width,\n",
    "        count=1,\n",
    "        dtype=rasterized.dtype,\n",
    "        crs=ref_crs,\n",
    "        transform=ref_transform,\n",
    "        nodata=2  # Set NoData value to 2\n",
    "    ) as dst:\n",
    "        dst.write(rasterized, 1)\n",
    "\n",
    "# Convert each field in each shapefile to a raster using the snapping method\n",
    "fields = ['DS', 'DF', 'ES', 'EF', 'RS']\n",
    "for shapefile in shapefiles:\n",
    "    for field in fields:\n",
    "        output_raster = os.path.join(output_dir, f\"{os.path.basename(shapefile).replace('.shp', '')}_{field}.tif\")\n",
    "        shapefile_to_raster_with_snap(shapefile, field, output_raster)\n",
    "        print(f\"Raster created with snapping: {output_raster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce the confusion matrix for the all landsldie tyep and configurations\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the base paths\n",
    "base_path_prob = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Results_GIS/\"\n",
    "base_path_GT = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Data/\"\n",
    "\n",
    "# List of probabilities shapefiles \n",
    "shapefiles_prob = [\n",
    "    f\"{base_path_prob}allProbs_hourly.shp\",\n",
    "    # f\"{base_path_prob}allProbs_daily_75.shp\", \n",
    "    # f\"{base_path_prob}allProbs_terrain_allCum.shp\", \n",
    "    # f\"{base_path_prob}allProbs_terrain_3wCum.shp\",\n",
    "    # f\"{base_path_prob}allProbs_terrain_2wCum.shp\", \n",
    "    # f\"{base_path_prob}allProbs_terrain_1wCum.shp\", \n",
    "    # f\"{base_path_prob}allProbs_terrain_MonCum.shp\", \n",
    "    # f\"{base_path_prob}allProbs_terrain.shp\"\n",
    "]\n",
    "\n",
    "# List of ground truth npy files\n",
    "npy_GT = [\n",
    "    f\"{base_path_GT}su05_LS_DS.npy\", \n",
    "    f\"{base_path_GT}su05_LS_DF.npy\", \n",
    "    f\"{base_path_GT}su05_LS_ES.npy\",\n",
    "    f\"{base_path_GT}su05_LS_EF.npy\", \n",
    "    f\"{base_path_GT}su05_LS_RS1.npy\",\n",
    "]\n",
    "\n",
    "# Load the ground truth data\n",
    "ground_truths = [np.load(gt_file) for gt_file in npy_GT]\n",
    "\n",
    "# Function to add confusion matrix field to a GeoDataFrame\n",
    "def add_confusion_matrix_fields(prob_gdf, gt_array, field, threshold=0.5):\n",
    "    # Create a new field for Confusion Matrix results\n",
    "    prob_gdf[f'{field}_CF'] = \"TP\"\n",
    "    \n",
    "    # Iterate over the GeoDataFrame and compare with the ground truth\n",
    "    for idx, row in prob_gdf.iterrows():\n",
    "        prob_value = 1 if row[field] >= threshold else 0  # Apply the threshold\n",
    "        gt_value = gt_array[idx]  # Get the corresponding ground truth value\n",
    "        \n",
    "        if prob_value == 1 and gt_value == 1:\n",
    "            prob_gdf.at[idx, f'{field}_CF'] = \"TP\"  # True Positive\n",
    "        elif prob_value == 0 and gt_value == 0:\n",
    "            prob_gdf.at[idx, f'{field}_CF'] = \"TN\"  # True Negative\n",
    "        elif prob_value == 1 and gt_value == 0:\n",
    "            prob_gdf.at[idx, f'{field}_CF'] = \"FP\"  # False Positive\n",
    "        elif prob_value == 0 and gt_value == 1:\n",
    "            prob_gdf.at[idx, f'{field}_CF'] = \"FN\"  # False Negative\n",
    "\n",
    "# Iterate through each probability shapefile (configuration)\n",
    "for shapefile_prob in shapefiles_prob:\n",
    "    # Load the probability shapefile\n",
    "    prob_gdf = gpd.read_file(shapefile_prob)\n",
    "    \n",
    "    # Ensure the GeoDataFrame is not empty\n",
    "    if prob_gdf.empty:\n",
    "        print(f\"Skipping empty shapefile: {shapefile_prob}\")\n",
    "        continue\n",
    "    \n",
    "    # Iterate through each landslide type field\n",
    "    for field, gt_array in zip(['DS', 'DF', 'ES', 'EF', 'RS'], ground_truths):\n",
    "        # Ensure the ground truth and shapefile are aligned\n",
    "        if len(prob_gdf) != len(gt_array):\n",
    "            raise ValueError(f\"Mismatch between the number of entries in {shapefile_prob} and ground truth array for {field}.\")\n",
    "        \n",
    "        # Add confusion matrix field with the threshold applied\n",
    "        add_confusion_matrix_fields(prob_gdf, gt_array, field, threshold=0.5)\n",
    "    \n",
    "    # Save the modified shapefile\n",
    "    output_shapefile = shapefile_prob.replace(\".shp\", \"_CF.shp\")\n",
    "    prob_gdf.to_file(output_shapefile)\n",
    "    print(f\"Confusion matrix shapefile created: {output_shapefile}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of presence absence FN and relative landslide area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # Import pandas for saving to Excel\n",
    "import openpyxl\n",
    "\n",
    "# Define the base paths\n",
    "base_path_CF = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Results_GIS/\"\n",
    "base_path_GT_area = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Data/\"\n",
    "\n",
    "# List of CF shapefiles (classification with Confusion Matrix results)\n",
    "shapefiles_CF = [\n",
    "    f\"{base_path_CF}allProbs_hourly_CF.shp\",\n",
    "    f\"{base_path_CF}allProbs_daily_75_CF.shp\", \n",
    "    f\"{base_path_CF}allProbs_terrain_allCum_CF.shp\", \n",
    "    f\"{base_path_CF}allProbs_terrain_3wCum_CF.shp\",\n",
    "    f\"{base_path_CF}allProbs_terrain_2wCum_CF.shp\", \n",
    "    f\"{base_path_CF}allProbs_terrain_1wCum_CF.shp\", \n",
    "    f\"{base_path_CF}allProbs_terrain_MonCum_CF.shp\", \n",
    "    f\"{base_path_CF}allProbs_terrain_CF.shp\"\n",
    "]\n",
    "\n",
    "# List of ground truth landslide shapefiles\n",
    "shapefiles_LS = [\n",
    "    f\"{base_path_GT_area}LS_DS.shp\", \n",
    "    f\"{base_path_GT_area}LS_DF.shp\", \n",
    "    f\"{base_path_GT_area}LS_ES.shp\",\n",
    "    f\"{base_path_GT_area}LS_EF.shp\", \n",
    "    f\"{base_path_GT_area}LS_RS1.shp\", \n",
    "]\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over CF shapefiles\n",
    "for cf_shapefile in shapefiles_CF:\n",
    "    \n",
    "    # Load the current CF shapefile (Confusion Matrix results)\n",
    "    cf_gdf = gpd.read_file(cf_shapefile)\n",
    "    \n",
    "    # Filter for only the 'TP' (True Positive) polygons in the CF shapefile\n",
    "    TP_DS = cf_gdf[cf_gdf['DS_CF'] == 'TP']  \n",
    "    TP_DF = cf_gdf[cf_gdf['DF_CF'] == 'TP']\n",
    "    TP_ES = cf_gdf[cf_gdf['ES_CF'] == 'TP']\n",
    "    TP_EF = cf_gdf[cf_gdf['EF_CF'] == 'TP']\n",
    "    TP_RS = cf_gdf[cf_gdf['RS_CF'] == 'TP']\n",
    "    \n",
    "    # Filter for positive (TP or FN) polygons\n",
    "    positive_DS = cf_gdf[cf_gdf['DS_CF'].isin(['TP', 'FN'])]\n",
    "    positive_DF = cf_gdf[cf_gdf['DF_CF'].isin(['TP', 'FN'])]\n",
    "    positive_ES = cf_gdf[cf_gdf['ES_CF'].isin(['TP', 'FN'])]\n",
    "    positive_EF = cf_gdf[cf_gdf['EF_CF'].isin(['TP', 'FN'])]\n",
    "    positive_RS = cf_gdf[cf_gdf['RS_CF'].isin(['TP', 'FN'])]\n",
    "\n",
    "    print(f\"\\nProcessing CF shapefile: {cf_shapefile}\")\n",
    "    \n",
    "    # Load the landslide shapefiles (ground truth)\n",
    "    ls_DS = gpd.read_file(shapefiles_LS[0])\n",
    "    ls_DF = gpd.read_file(shapefiles_LS[1])  \n",
    "    ls_ES = gpd.read_file(shapefiles_LS[2])\n",
    "    ls_EF = gpd.read_file(shapefiles_LS[3])\n",
    "    ls_RS = gpd.read_file(shapefiles_LS[4])\n",
    "\n",
    "    # Calculate total area of each landslide shapefile (ground truth)\n",
    "    total_area_ls_DS = ls_DS.geometry.area.sum()\n",
    "    total_area_ls_DF = ls_DF.geometry.area.sum()\n",
    "    total_area_ls_ES = ls_ES.geometry.area.sum()\n",
    "    total_area_ls_EF = ls_EF.geometry.area.sum()\n",
    "    total_area_ls_RS = ls_RS.geometry.area.sum()\n",
    "\n",
    "    # Perform spatial intersection between TP polygons and landslide polygons\n",
    "    intersection_DS = gpd.overlay(ls_DS, TP_DS, how='intersection')\n",
    "    intersection_DF = gpd.overlay(ls_DF, TP_DF, how='intersection')\n",
    "    intersection_ES = gpd.overlay(ls_ES, TP_ES, how='intersection')\n",
    "    intersection_EF = gpd.overlay(ls_EF, TP_EF, how='intersection')\n",
    "    intersection_RS = gpd.overlay(ls_RS, TP_RS, how='intersection')\n",
    "\n",
    "    # Calculate the area of the intersection for each type\n",
    "    intersection_DS['area'] = intersection_DS.geometry.area\n",
    "    intersection_DF['area'] = intersection_DF.geometry.area\n",
    "    intersection_ES['area'] = intersection_ES.geometry.area\n",
    "    intersection_EF['area'] = intersection_EF.geometry.area\n",
    "    intersection_RS['area'] = intersection_RS.geometry.area\n",
    "    \n",
    "    # Calculate total TP area for each type\n",
    "    total_area_TP_DS = intersection_DS['area'].sum()\n",
    "    total_area_TP_DF = intersection_DF['area'].sum()\n",
    "    total_area_TP_ES = intersection_ES['area'].sum()\n",
    "    total_area_TP_EF = intersection_EF['area'].sum()\n",
    "    total_area_TP_RS = intersection_RS['area'].sum()\n",
    "\n",
    "    # Calculate the ratio of TP area to total landslide area for each type\n",
    "    ratio_DS = total_area_TP_DS / total_area_ls_DS if total_area_ls_DS > 0 else 0\n",
    "    ratio_DF = total_area_TP_DF / total_area_ls_DF if total_area_ls_DF > 0 else 0\n",
    "    ratio_ES = total_area_TP_ES / total_area_ls_ES if total_area_ls_ES > 0 else 0\n",
    "    ratio_EF = total_area_TP_EF / total_area_ls_EF if total_area_ls_EF > 0 else 0\n",
    "    ratio_RS = total_area_TP_RS / total_area_ls_RS if total_area_ls_RS > 0 else 0\n",
    "\n",
    "    # Calculate the count of TP and total positive slope units (TP + FN)\n",
    "    count_TP_DS = len(TP_DS)\n",
    "    count_positive_DS = len(positive_DS)\n",
    "    ratio_count_DS = count_TP_DS / count_positive_DS if count_positive_DS > 0 else 0\n",
    "\n",
    "    count_TP_DF = len(TP_DF)\n",
    "    count_positive_DF = len(positive_DF)\n",
    "    ratio_count_DF = count_TP_DF / count_positive_DF if count_positive_DF > 0 else 0\n",
    "\n",
    "    count_TP_ES = len(TP_ES)\n",
    "    count_positive_ES = len(positive_ES)\n",
    "    ratio_count_ES = count_TP_ES / count_positive_ES if count_positive_ES > 0 else 0\n",
    "\n",
    "    count_TP_EF = len(TP_EF)\n",
    "    count_positive_EF = len(positive_EF)\n",
    "    ratio_count_EF = count_TP_EF / count_positive_EF if count_positive_EF > 0 else 0\n",
    "\n",
    "    count_TP_RS = len(TP_RS)\n",
    "    count_positive_RS = len(positive_RS)\n",
    "    ratio_count_RS = count_TP_RS / count_positive_RS if count_positive_RS > 0 else 0\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append([\n",
    "        os.path.basename(cf_shapefile), \n",
    "        total_area_TP_DS, ratio_DS, count_TP_DS, ratio_count_DS,\n",
    "        total_area_TP_DF, ratio_DF, count_TP_DF, ratio_count_DF,\n",
    "        total_area_TP_ES, ratio_ES, count_TP_ES, ratio_count_ES,\n",
    "        total_area_TP_EF, ratio_EF, count_TP_EF, ratio_count_EF,\n",
    "        total_area_TP_RS, ratio_RS, count_TP_RS, ratio_count_RS\n",
    "    ])\n",
    "\n",
    "# Convert results to a numpy array\n",
    "results_array = np.array(results, dtype=object)\n",
    "\n",
    "# Save the results to a .npy file\n",
    "output_npy = os.path.join(base_path_CF, \"TP_area_ratio_summary.npy\")\n",
    "np.save(output_npy, results_array)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_excel = os.path.join(base_path_CF, \"TP_area_ratio_summary.xlsx\")\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    'Configuration', \n",
    "    'Total_area_TP_DS', 'Area_Ratio_DS', 'Total_count_TP_DS', 'Count_Ratio_DS',\n",
    "    'Total_area_TP_DF', 'Area_Ratio_DF', 'Total_count_TP_DF', 'Count_Ratio_DF',\n",
    "    'Total_area_TP_ES', 'Area_Ratio_ES', 'Total_count_TP_ES', 'Count_Ratio_ES',\n",
    "    'Total_area_TP_EF', 'Area_Ratio_EF', 'Total_count_TP_EF', 'Count_Ratio_EF',\n",
    "    'Total_area_TP_RS', 'Area_Ratio_RS', 'Total_count_TP_RS', 'Count_Ratio_RS'\n",
    "])\n",
    "df.to_excel(output_excel, index=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFinal Results (TP Polygon Areas, Ratios, Counts inside Landslide Shapefiles):\")\n",
    "print(\"CF Shapefile, Total_TP_DS, Ratio_DS, Count_TP_DS, Ratio_Count_DS, Total_TP_DF, Ratio_DF, Count_TP_DF, Ratio_Count_DF, Total_TP_ES, Ratio_ES, Count_TP_ES, Ratio_Count_ES, Total_TP_EF, Ratio_EF, Count_TP_EF, Ratio_Count_EF, Total_TP_RS, Ratio_RS, Count_TP_RS, Ratio_Count_RS\")\n",
    "for row in results_array:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the ls counts and binary probability results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pyogrio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path where your shapefiles are located\n",
    "base_path = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Results_GIS/\"\n",
    "\n",
    "# Load your source shapefiles and normalize the 'Trig_count' field\n",
    "def normalize_trig_count(shapefile_path, output_field):\n",
    "    # Read shapefile into a GeoDataFrame\n",
    "    gdf = pyogrio.read_dataframe(shapefile_path)\n",
    "    \n",
    "    # Normalize 'Trig_count' field\n",
    "    scaler = MinMaxScaler()\n",
    "    gdf[output_field] = scaler.fit_transform(gdf[['Trig_count']])\n",
    "    \n",
    "    return gdf[['value', output_field]]\n",
    "\n",
    "# List of shapefiles and corresponding fields\n",
    "shapefiles = {\n",
    "    f\"{base_path}su05__LS_DS.shp\": \"DensNor_DS\",\n",
    "    f\"{base_path}su05__LS_DF.shp\": \"DensNor_DF\",\n",
    "    f\"{base_path}su05__LS_ES.shp\": \"DensNor_ES\",\n",
    "    f\"{base_path}su05__LS_EF.shp\": \"DensNor_EF\",\n",
    "    f\"{base_path}su05__LS_RS1.shp\": \"DensNor_RS\"\n",
    "}\n",
    "\n",
    "# Normalize each shapefile and store the results in a dictionary\n",
    "normalized_data = {}\n",
    "for shapefile, output_field in shapefiles.items():\n",
    "    normalized_data[output_field] = normalize_trig_count(shapefile, output_field)\n",
    "\n",
    "# List of destination shapefiles\n",
    "dest_shapefiles = [\n",
    "    f\"{base_path}allProbs_daily_75.shp\", \n",
    "    f\"{base_path}allProbs_terrain_allCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_3wCum.shp\",\n",
    "    f\"{base_path}allProbs_terrain_2wCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_1wCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_MonCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain.shp\"\n",
    "]\n",
    "\n",
    "# Function to add normalized fields and calculate differences\n",
    "def add_fields_and_calculate_diff(dest_shapefile, normalized_data):\n",
    "    # Read the destination shapefile into a GeoDataFrame\n",
    "    dest_gdf = pyogrio.read_dataframe(dest_shapefile)\n",
    "    print(f\"Initial columns in {dest_shapefile}: {dest_gdf.columns.tolist()}\")\n",
    "    \n",
    "    # Merge the normalized data based on the 'value' field\n",
    "    for norm_field, norm_gdf in normalized_data.items():\n",
    "        print(f\"Merging {norm_field} into {dest_shapefile}\")\n",
    "        dest_gdf = dest_gdf.merge(norm_gdf, on='value', how='left')\n",
    "        print(f\"Columns after merging {norm_field}: {dest_gdf.columns.tolist()}\")\n",
    "        # Handle potential field name conflicts\n",
    "        if f\"{norm_field}_x\" in dest_gdf.columns:\n",
    "            dest_gdf[norm_field] = dest_gdf[f\"{norm_field}_x\"]\n",
    "            dest_gdf.drop(columns=[f\"{norm_field}_x\", f\"{norm_field}_y\"], inplace=True)\n",
    "        elif f\"{norm_field}_y\" in dest_gdf.columns:\n",
    "            dest_gdf[norm_field] = dest_gdf[f\"{norm_field}_y\"]\n",
    "            dest_gdf.drop(columns=[f\"{norm_field}_y\"], inplace=True)\n",
    "    \n",
    "    # Calculate the difference fields\n",
    "    for original_field, norm_field in zip(['DS', 'DF', 'ES', 'EF', 'RS'], shapefiles.values()):\n",
    "        diff_field = f\"{original_field}_diff_GT\"\n",
    "        if norm_field in dest_gdf.columns:\n",
    "            dest_gdf[diff_field] = dest_gdf[original_field] - dest_gdf[norm_field]\n",
    "            print(f\"Calculated {diff_field} in {dest_shapefile}\")\n",
    "        else:\n",
    "            print(f\"Error: {norm_field} not found in {dest_shapefile} during calculation of {diff_field}\")\n",
    "    \n",
    "    # Save the updated GeoDataFrame back to a shapefile\n",
    "    pyogrio.write_dataframe(dest_gdf, dest_shapefile)\n",
    "\n",
    "# Apply the process to all destination shapefiles\n",
    "for dest_shapefile in dest_shapefiles:\n",
    "    add_fields_and_calculate_diff(dest_shapefile, normalized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyogrio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the base path where your shapefiles are located\n",
    "base_path = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Results_GIS/\"\n",
    "\n",
    "# List of destination shapefiles\n",
    "dest_shapefiles = [\n",
    "    f\"{base_path}allProbs_daily_75.shp\", \n",
    "    f\"{base_path}allProbs_terrain_allCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_3wCum.shp\",\n",
    "    f\"{base_path}allProbs_terrain_2wCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_1wCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_MonCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain.shp\"\n",
    "]\n",
    "\n",
    "# Function to compute statistics for the *_diff_GT fields\n",
    "def compute_statistics(diff_fields):\n",
    "    stats = {}\n",
    "    for field in diff_fields:\n",
    "        stats[field] = {\n",
    "            \"min\": diff_fields[field].min(),\n",
    "            \"max\": diff_fields[field].max(),\n",
    "            \"mean\": diff_fields[field].mean(),\n",
    "            \"std_dev\": diff_fields[field].std(),\n",
    "            \"25_percentile\": np.percentile(diff_fields[field], 25),\n",
    "            \"75_percentile\": np.percentile(diff_fields[field], 75)\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "# Iterate over each shapefile and compute statistics for *_diff_GT fields\n",
    "for dest_shapefile in dest_shapefiles:\n",
    "    # Read the shapefile into a GeoDataFrame\n",
    "    dest_gdf = pyogrio.read_dataframe(dest_shapefile)\n",
    "    \n",
    "    # Filter columns to get only *_diff_GT fields\n",
    "    diff_gt_fields = [col for col in dest_gdf.columns if col.endswith('_diff_GT')]\n",
    "    \n",
    "    # Extract the data for these fields\n",
    "    diff_data = dest_gdf[diff_gt_fields]\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats = compute_statistics(diff_data)\n",
    "    \n",
    "    # Print the statistics for each shapefile\n",
    "    print(f\"Statistics for {dest_shapefile}:\")\n",
    "    for field, field_stats in stats.items():\n",
    "        print(f\"Field: {field}\")\n",
    "        for stat_name, value in field_stats.items():\n",
    "            print(f\"  {stat_name}: {value}\")\n",
    "    print(\"\\n\")  # Add a newline for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyogrio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the base path where your shapefiles are located\n",
    "base_path = \"Z:/GEOAPP_Synology/Lavori/Twente/Python_twente/ashokdahal-TransformerLandslide-23fdcf6/Results_GIS/\"\n",
    "\n",
    "# List of destination shapefiles\n",
    "dest_shapefiles = [\n",
    "    f\"{base_path}allProbs_daily_75.shp\", \n",
    "    f\"{base_path}allProbs_terrain_allCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_3wCum.shp\",\n",
    "    f\"{base_path}allProbs_terrain_2wCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_1wCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain_MonCum.shp\", \n",
    "    f\"{base_path}allProbs_terrain.shp\"\n",
    "]\n",
    "\n",
    "# Function to compute error metrics\n",
    "def compute_error_metrics(pred, obs):\n",
    "    mae = np.mean(np.abs(pred - obs))\n",
    "    rmse = np.sqrt(np.mean((pred - obs) ** 2))\n",
    "    bias = np.mean(pred - obs)\n",
    "    obs_mean = np.mean(obs)\n",
    "    \n",
    "    rae = np.sum(np.abs(pred - obs)) / np.sum(np.abs(obs - obs_mean))\n",
    "    rse = np.sum((pred - obs) ** 2) / np.sum((obs - obs_mean) ** 2)\n",
    "    \n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Bias\": bias,\n",
    "        \"RAE\": rae,\n",
    "        \"RSE\": rse\n",
    "    }\n",
    "\n",
    "# Create a dictionary to store metrics for each shapefile\n",
    "metrics_summary = {}\n",
    "\n",
    "# Iterate over each shapefile and compute error metrics for *_diff_GT fields\n",
    "for dest_shapefile in dest_shapefiles:\n",
    "    # Initialize the dictionary for the current shapefile\n",
    "    metrics_summary[dest_shapefile] = {}\n",
    "    \n",
    "    # Read the shapefile into a GeoDataFrame\n",
    "    dest_gdf = pyogrio.read_dataframe(dest_shapefile)\n",
    "    \n",
    "    # Filter columns to get only *_diff_GT fields\n",
    "    diff_gt_fields = [col for col in dest_gdf.columns if col.endswith('_diff_GT')]\n",
    "    \n",
    "    for diff_field in diff_gt_fields:\n",
    "        # Extract the predicted and observed values\n",
    "        pred_field = diff_field.replace(\"_diff_GT\", \"\")  # e.g., DS_diff_GT -> DS\n",
    "        observed_field = \"DensNor_\" + pred_field  # Construct the expected observed field name\n",
    "        \n",
    "        if pred_field in dest_gdf.columns and observed_field in dest_gdf.columns:\n",
    "            pred = dest_gdf[pred_field]\n",
    "            obs = dest_gdf[observed_field]  # Observed values should be in DensNor_DS, etc.\n",
    "            \n",
    "            # Compute error metrics\n",
    "            error_metrics = compute_error_metrics(pred, obs)\n",
    "            metrics_summary[dest_shapefile][pred_field] = error_metrics\n",
    "        else:\n",
    "            if pred_field not in dest_gdf.columns:\n",
    "                print(f\"Warning: {pred_field} not found in {dest_shapefile}\")\n",
    "            if observed_field not in dest_gdf.columns:\n",
    "                print(f\"Warning: {observed_field} not found in {dest_shapefile}\")\n",
    "\n",
    "# Calculate average metrics for each shapefile\n",
    "average_metrics = {}\n",
    "for shapefile, landslide_types in metrics_summary.items():\n",
    "    avg_mae = np.mean([v[\"MAE\"] for v in landslide_types.values()])\n",
    "    avg_rmse = np.mean([v[\"RMSE\"] for v in landslide_types.values()])\n",
    "    avg_bias = np.mean([v[\"Bias\"] for v in landslide_types.values()])\n",
    "    avg_rae = np.mean([v[\"RAE\"] for v in landslide_types.values()])\n",
    "    avg_rse = np.mean([v[\"RSE\"] for v in landslide_types.values()])\n",
    "    \n",
    "    average_metrics[shapefile] = {\n",
    "        \"Avg MAE\": avg_mae,\n",
    "        \"Avg RMSE\": avg_rmse,\n",
    "        \"Avg Bias\": avg_bias,\n",
    "        \"Avg RAE\": avg_rae,\n",
    "        \"Avg RSE\": avg_rse\n",
    "    }\n",
    "\n",
    "# Find the best shapefile overall\n",
    "best_shapefile_overall = min(average_metrics, key=lambda k: (average_metrics[k][\"Avg RMSE\"], average_metrics[k][\"Avg MAE\"]))\n",
    "print(f\"Best overall shapefile: {best_shapefile_overall} with metrics: {average_metrics[best_shapefile_overall]}\")\n",
    "\n",
    "# Find the best shapefile for each landslide type\n",
    "best_shapefiles_per_type = {}\n",
    "for landslide_type in [\"DS\", \"DF\", \"ES\", \"EF\", \"RS\"]:\n",
    "    best_shapefile = min(metrics_summary, key=lambda k: (metrics_summary[k][landslide_type][\"RMSE\"], metrics_summary[k][landslide_type][\"MAE\"]))\n",
    "    best_shapefiles_per_type[landslide_type] = best_shapefile\n",
    "\n",
    "print(\"Best shapefile per landslide type:\")\n",
    "for landslide_type, shapefile in best_shapefiles_per_type.items():\n",
    "    print(f\"  {landslide_type}: {shapefile} with metrics: {metrics_summary[shapefile][landslide_type]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best shapefile overall\n",
    "best_shapefile_overall = min(average_metrics, key=lambda k: (average_metrics[k][\"Avg RMSE\"], average_metrics[k][\"Avg Bias\"]))\n",
    "print(f\"Best overall shapefile: {best_shapefile_overall} with metrics: {average_metrics[best_shapefile_overall]}\")\n",
    "\n",
    "# Find the best shapefile for each landslide type\n",
    "best_shapefiles_per_type = {}\n",
    "for landslide_type in [\"DS\", \"DF\", \"ES\", \"EF\", \"RS\"]:\n",
    "    best_shapefile = min(metrics_summary, key=lambda k: (metrics_summary[k][landslide_type][\"RMSE\"], metrics_summary[k][landslide_type][\"Bias\"]))\n",
    "    best_shapefiles_per_type[landslide_type] = best_shapefile\n",
    "\n",
    "print(\"Best shapefile per landslide type:\")\n",
    "for landslide_type, shapefile in best_shapefiles_per_type.items():\n",
    "    print(f\"  {landslide_type}: {shapefile} with metrics: {metrics_summary[shapefile][landslide_type]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the metrics and calculate the composite score for each landslide type\n",
    "composite_scores_per_type = {lt: {} for lt in [\"DS\", \"DF\", \"ES\", \"EF\", \"RS\"]}\n",
    "\n",
    "for landslide_type in [\"DS\", \"DF\", \"ES\", \"EF\", \"RS\"]:\n",
    "    # Extract metrics for this landslide type across all shapefiles\n",
    "    metrics_per_type = {shp: metrics_summary[shp][landslide_type] for shp in dest_shapefiles if landslide_type in metrics_summary[shp]}\n",
    "    \n",
    "    # Normalize metrics\n",
    "    for metric in [\"MAE\", \"RMSE\", \"Bias\", \"RAE\", \"RSE\"]:\n",
    "        values = [metrics[metric] for metrics in metrics_per_type.values()]\n",
    "        min_value, max_value = min(values), max(values)\n",
    "        for shp in metrics_per_type:\n",
    "            norm_value = (metrics_per_type[shp][metric] - min_value) / (max_value - min_value) if max_value != min_value else 0\n",
    "            metrics_per_type[shp][f\"norm_{metric}\"] = norm_value\n",
    "    \n",
    "    # Calculate composite score\n",
    "    for shp in metrics_per_type:\n",
    "        composite_score = np.mean([metrics_per_type[shp][f\"norm_{metric}\"] for metric in [\"MAE\", \"RMSE\", \"Bias\", \"RAE\", \"RSE\"]])\n",
    "        composite_scores_per_type[landslide_type][shp] = composite_score\n",
    "\n",
    "# Identify the best shapefile for each landslide type based on the composite score\n",
    "best_shapefiles_per_type = {}\n",
    "for landslide_type, scores in composite_scores_per_type.items():\n",
    "    best_shapefile = min(scores, key=scores.get)\n",
    "    best_shapefiles_per_type[landslide_type] = best_shapefile\n",
    "\n",
    "# Print the best shapefile for each landslide type\n",
    "print(\"Best shapefile per landslide type based on composite score:\")\n",
    "for landslide_type, shapefile in best_shapefiles_per_type.items():\n",
    "    print(f\"  {landslide_type}: {shapefile} with composite score: {composite_scores_per_type[landslide_type][shapefile]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twente_old_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
