{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THIS FOR PREDICTION, FOR PLOTS DIRECTLY RUN BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from src import preparedata\n",
    "from src import transformermodel\n",
    "from src import traintransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "import pyogrio\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TIME series VARABLES (hourly RAIN) and COSTANT TERRAIN\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/params.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# # FOR TEST (put inference fasle)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/DS_hourly_75.h5\")\n",
    "# # model = tf.keras.models.load_model(\"checkpoints/DS_hourly_75.keras\")\n",
    "# preds = model.predict([dataset.Xt_test, dataset.Xc_test], batch_size = 100)\n",
    "\n",
    "# np.save(\"Data/DS_hourly_all_pred_test.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/DS_hourly_all_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "landslidehazard = transformermodel.lsmodel(params[\"modelparam\"])\n",
    "landslidehazard.preparemodel()\n",
    "landslidehazard.model.load_weights(\"checkpoints/RS1_hourly_75.h5\")\n",
    "\n",
    "# model = tf.keras.models.load_model(\"checkpoints/DS_hourly_75.h5\")\n",
    "preds = landslidehazard.model.predict([dataset.Xt, dataset.Xc], batch_size = 100)\n",
    "\n",
    "np.save(\"Data/RS_hourly_all_pred.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/RS_hourly_all_Y_test.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Xt (time series data): 69159\n",
      "Length of Xc (constant terrain data): 69159\n",
      "Length of Y (response data): 69159\n",
      "\u001b[1m346/346\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step\n",
      "Length of pred (predicted data): 69159\n"
     ]
    }
   ],
   "source": [
    "# ROR TIME series VARABLES (daily RAIN) and COSTANT TERRAIN\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/params.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# # FOR TEST (put inference fasle)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/EF_daily_75.keras\")\n",
    "# preds = model.predict([dataset.Xt_test, dataset.Xc_test], batch_size = 100)\n",
    "\n",
    "# np.save(\"Data/EF_daily_75_pred_test.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/EF_daily_75_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "model = tf.keras.models.load_model(\"checkpoints/RS_daily_75.keras\")\n",
    "preds = model.predict([dataset.Xt, dataset.Xc], batch_size = 200)\n",
    "\n",
    "np.save(\"Data/RS_daily_75_pred_sep24.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/RS_daily_75_Y_test_sep24.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/paramsTerrain.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# FOR TEST (put inference fasle)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "model = tf.keras.models.load_model(\"checkpoints/RS_terrain.keras\")\n",
    "preds = model.predict(dataset.Xc_test, batch_size=100)\n",
    "\n",
    "np.save(\"Data/RS_terrain_pred_test.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predocted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/RS_terrain_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "# ## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/DS_terrain.keras\")\n",
    "# preds = model.predict(dataset.Xc, batch_size=100)\n",
    "\n",
    "# np.save(\"Data/RS_terrain_pred.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/RS_terrain_Y_test.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + MONTH scalar rain (cumulative)\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/paramsTerrain.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# FOR TEST (put inference fasle)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "model = tf.keras.models.load_model(\"checkpoints/RS_terrain_Moncum.keras\")\n",
    "preds = model.predict(dataset.Xc_test, batch_size=100)\n",
    "\n",
    "np.save(\"Data/RS_terrain_MonCum_pred_test.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predocted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/RS_terrain_MonCum_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "# ## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/RS_terrain_Moncum.keras\")\n",
    "# preds = model.predict(dataset.Xc, batch_size=100)\n",
    "\n",
    "# np.save(\"Data/RS_terrain_MonCum_pred.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/RS_terrain_MonCum_Y_test.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + last 3 weeks scalar rain (cumulative)\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/paramsTerrain.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# FOR TEST (put inference fasle)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "model = tf.keras.models.load_model(\"checkpoints/RS_terrain_3wcum.keras\")\n",
    "preds = model.predict(dataset.Xc_test, batch_size=100)\n",
    "\n",
    "np.save(\"Data/RS_terrain_3wCum_pred_test.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predocted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/RS_terrain_3wCum_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "# ## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/RS_terrain_3wcum.keras\")\n",
    "# preds = model.predict(dataset.Xc, batch_size=100)\n",
    "\n",
    "# np.save(\"Data/RS_terrain_3wCum_pred.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/RS_terrain_3wCum_Y_test.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + last 2 weeks scalar rain (cumulative)\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/paramsTerrain.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# FOR TEST (put inference fasle)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "model = tf.keras.models.load_model(\"checkpoints/EF_terrain_2wcum.keras\")\n",
    "preds = model.predict(dataset.Xc_test, batch_size=100)\n",
    "\n",
    "np.save(\"Data/EF_terrain_2wCum_pred_test.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predocted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/EF_terrain_2wCum_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "# ## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/RS_terrain_2wcum.keras\")\n",
    "# preds = model.predict(dataset.Xc, batch_size=100)\n",
    "\n",
    "# np.save(\"Data/RS_terrain_2wCum_pred.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/RS_terrain_2wCum_Y_test.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + last 1 weeks scalar rain (cumulative)\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/paramsTerrain.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# FOR TEST (put inference fasle)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "model = tf.keras.models.load_model(\"checkpoints/RS_terrain_1wcum.keras\")\n",
    "preds = model.predict(dataset.Xc_test, batch_size=100)\n",
    "\n",
    "np.save(\"Data/RS_terrain_1wCum_pred_test.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predocted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/RS_terrain_1wCum_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "# ## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/RS_terrain_1wcum.keras\")\n",
    "# preds = model.predict(dataset.Xc, batch_size=100)\n",
    "\n",
    "# np.save(\"Data/RS_terrain_1wCum_pred.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/RS_terrain_1wCum_Y_test.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + all weeeks and month scalar rain (cumulative)\n",
    "\n",
    "# Change the dataset input in params for save the prediction of all landslide type \n",
    "#  first PUT INFERENCE ON True and remuvezero on false for prediction in the whole area\n",
    "#  then PUT INFERENCE OFF to plot the metrics and modify below to adding _test to the dataset in prediction\n",
    "\n",
    "params = json.load(open(\"params/paramsTerrain.json\", \"r\"))\n",
    "dataset = preparedata.readTransformerData(params[\"dataprepinargs\"])\n",
    "dataset.preparedata()\n",
    "\n",
    "# FOR TEST (put inference fasle)\n",
    "# Print lengths of the original data\n",
    "print(\"Length of Xt (time series data):\", len(dataset.Xt_test))\n",
    "print(\"Length of Xc (constant terrain data):\", len(dataset.Xc_test))\n",
    "print(\"Length of Y (response data):\", len(dataset.Y_test))\n",
    "\n",
    "# predict with timeseriesandconstant\n",
    "model = tf.keras.models.load_model(\"checkpoints/RS_terrain_allcum.keras\")\n",
    "preds = model.predict(dataset.Xc_test, batch_size=100)\n",
    "\n",
    "np.save(\"Data/RS_terrain_allCum_pred_test.npy\", preds)\n",
    "\n",
    "print(\"Length of pred (predocted data):\", len(preds))\n",
    "\n",
    "np.save(\"Data/RS_terrain_allCum_Y_test_test.npy\", dataset.Y_test)\n",
    "\n",
    "# ## FOR SAVE PREDICTIONS WHOLE AREA (put inference true)\n",
    "# # Print lengths of the original data\n",
    "# print(\"Length of Xt (time series data):\", len(dataset.Xt))\n",
    "# print(\"Length of Xc (constant terrain data):\", len(dataset.Xc))\n",
    "# print(\"Length of Y (response data):\", len(dataset.Y))\n",
    "\n",
    "# # predict with timeseriesandconstant\n",
    "# model = tf.keras.models.load_model(\"checkpoints/RS_terrain_allcum.keras\")\n",
    "# preds = model.predict(dataset.Xc, batch_size=100)\n",
    "\n",
    "# np.save(\"Data/RS_terrain_allCum_pred.npy\", preds)\n",
    "\n",
    "# print(\"Length of pred (predicted data):\", len(preds))\n",
    "\n",
    "# np.save(\"Data/RS_terrain_allCum_Y_test.npy\", dataset.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving predicitons in shp with inference ON for the whole area and plot for different configuration comparing ls types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TIME SERIES VARABLES (hourly RAIN) and COSTANT TERRAIN\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(4, 4), dpi=500)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_hourly_all_Y_test.npy\",\n",
    "    \"Data/DF_hourly_all_Y_test.npy\",\n",
    "    \"Data/ES_hourly_all_Y_test.npy\",\n",
    "    \"Data/EF_hourly_all_Y_test.npy\",\n",
    "    \"Data/RS_hourly_all_Y_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_hourly_all_pred.npy\",\n",
    "    \"Data/DF_hourly_all_pred.npy\",\n",
    "    \"Data/ES_hourly_all_pred.npy\",\n",
    "    \"Data/EF_hourly_all_pred.npy\",\n",
    "    \"Data/RS_hourly_all_pred.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "best_thresholds = []\n",
    "max_mccs = []\n",
    "balanced_accuracies = []\n",
    "auc_scores = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    print(\"Length of pred (predicted data):\", len(Ydata))\n",
    "    print(\"Length of pred (predicted data):\", len(pred))\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate MCC for each threshold\n",
    "    mccs = []\n",
    "    for threshold in thresholds:\n",
    "        pred_binary = pred > threshold\n",
    "        mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "        mccs.append(mcc)\n",
    "    \n",
    "    # Find the threshold that maximizes MCC\n",
    "    max_mcc = max(mccs)\n",
    "    best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    pred_binary_best = pred > best_threshold\n",
    "    acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "    recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "    f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "    f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "    # Store the results for best threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "    max_mccs.append(max_mcc)\n",
    "    balanced_accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    f2_scores.append(f2)\n",
    "    kappa_scores.append(kappa)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_balanced_accuracies.append(fixed_acc)\n",
    "    fixed_recalls.append(fixed_recall)\n",
    "    fixed_mccs.append(fixed_mcc)\n",
    "    fixed_f1_scores.append(fixed_f1)\n",
    "    fixed_f2_scores.append(fixed_f2)\n",
    "    fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.axis(\"square\")\n",
    "plt.savefig(\"Plots/roc_hourly.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame to store the results using best threshold from MCC\n",
    "results_mcc_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (MCC)\": best_thresholds,\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": balanced_accuracies,\n",
    "    \"Recall\": recalls,\n",
    "    \"MCC\": max_mccs,\n",
    "    \"Kappa\": kappa_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"F2\": f2_scores\n",
    "})\n",
    "\n",
    "# Create DataFrame to store the results using fixed threshold 0.5\n",
    "results_fixed_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": fixed_balanced_accuracies,\n",
    "    \"Recall\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the results as CSV files\n",
    "#results_mcc_df.to_csv(\"Plots/results_mcc_hourly_75.csv\", index=False)\n",
    "#results_fixed_df.to_csv(\"Plots/results_fixed_hourly_75.csv\", index=False)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Results using best threshold (MCC):\")\n",
    "print(results_mcc_df)\n",
    "print(\"\\nResults using fixed threshold (0.5):\")\n",
    "print(results_fixed_df)\n",
    "\n",
    "\n",
    "# SAVE THE PREDICTS with time\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_hourly.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FOR TIME SERIES VARABLES (daily RAIN) and COSTANT TERRAIN\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "import geopandas as gpd\n",
    "\n",
    "figure(figsize=(4, 4), dpi=600)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_daily_75_Y_test_sep24.npy\",\n",
    "    \"Data/DF_daily_75_Y_test_sep24.npy\",\n",
    "    \"Data/ES_daily_75_Y_test_sep24.npy\",\n",
    "    \"Data/EF_daily_75_Y_test_sep24.npy\",\n",
    "    \"Data/RS_daily_75_Y_test_sep24.npy\",\n",
    "]\n",
    "\n",
    "preds = [\n",
    "    \"Data/DS_daily_75_pred_sep24.npy\",\n",
    "    \"Data/DF_daily_75_pred_sep24.npy\",\n",
    "    \"Data/ES_daily_75_pred_sep24.npy\",\n",
    "    \"Data/EF_daily_75_pred_sep24.npy\",\n",
    "    \"Data/RS_daily_75_pred_sep24.npy\",\n",
    "]\n",
    "\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# UNCOOMMENT FOR THE EVALUATION USING THE ACTUAL RAINFALL OF THE EVENT\n",
    "\n",
    "# # Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "# best_thresholds = []\n",
    "# max_mccs = []\n",
    "# balanced_accuracies = []\n",
    "# auc_scores = []\n",
    "# recalls = []\n",
    "# f1_scores = []\n",
    "# f2_scores = []\n",
    "# kappa_scores = []\n",
    "\n",
    "# # Lists to store metrics using fixed threshold 0.5\n",
    "# fixed_balanced_accuracies = []\n",
    "# fixed_recalls = []\n",
    "# fixed_mccs = []\n",
    "# fixed_kappa_scores = []\n",
    "# fixed_f1_scores = []\n",
    "# fixed_f2_scores = []\n",
    "\n",
    "# for i in range(len(names)):\n",
    "#     Ydata = np.load(y_data[i])\n",
    "#     pred = np.load(preds[i])\n",
    "#     print(\"Length of pred (predicted data):\", len(Ydata))\n",
    "#     print(\"Length of pred (predicted data):\", len(pred))\n",
    "#     fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "#     auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "#     # Calculate MCC for each threshold\n",
    "#     mccs = []\n",
    "#     for threshold in thresholds:\n",
    "#         pred_binary = pred > threshold\n",
    "#         mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "#         mccs.append(mcc)\n",
    "    \n",
    "#     # Find the threshold that maximizes MCC\n",
    "#     max_mcc = max(mccs)\n",
    "#     best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "#     # Calculate metrics using the best threshold\n",
    "#     pred_binary_best = pred > best_threshold\n",
    "#     acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "#     recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "#     f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "#     f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "#     kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "#     # Store the results for best threshold\n",
    "#     best_thresholds.append(best_threshold)\n",
    "#     max_mccs.append(max_mcc)\n",
    "#     balanced_accuracies.append(acc)\n",
    "#     auc_scores.append(auc)\n",
    "#     recalls.append(recall)\n",
    "#     f1_scores.append(f1)\n",
    "#     f2_scores.append(f2)\n",
    "#     kappa_scores.append(kappa)\n",
    "    \n",
    "#     # Calculate metrics using fixed threshold 0.5\n",
    "#     pred_binary_fixed = pred > 0.5\n",
    "#     fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "#     fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "#     fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "#     fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "#     fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "#     fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "#     # Store the results for fixed threshold\n",
    "#     fixed_balanced_accuracies.append(fixed_acc)\n",
    "#     fixed_recalls.append(fixed_recall)\n",
    "#     fixed_mccs.append(fixed_mcc)\n",
    "#     fixed_f1_scores.append(fixed_f1)\n",
    "#     fixed_f2_scores.append(fixed_f2)\n",
    "#     fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "#     plt.plot(\n",
    "#         fpr,\n",
    "#         tpr,\n",
    "#         color=colors[i],\n",
    "#         lw=lw,\n",
    "#         label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "#     )\n",
    "\n",
    "# ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.0])\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "# plt.axis(\"square\")\n",
    "# #plt.savefig(\"Plots/roc_daily_75.pdf\")\n",
    "# plt.show()\n",
    "\n",
    "# # Create DataFrame to store the results using best threshold from MCC\n",
    "# results_mcc_df = pd.DataFrame({\n",
    "#     \"LS type\": names,\n",
    "#     \"Best threshold (MCC)\": best_thresholds,\n",
    "#     \"AUC\": auc_scores,\n",
    "#     \"Accuracy\": balanced_accuracies,\n",
    "#     \"Recall\": recalls,\n",
    "#     \"MCC\": max_mccs,\n",
    "#     \"Kappa\": kappa_scores,\n",
    "#     \"F1\": f1_scores,\n",
    "#     \"F2\": f2_scores\n",
    "# })\n",
    "\n",
    "# # Create DataFrame to store the results using fixed threshold 0.5\n",
    "# results_fixed_df = pd.DataFrame({\n",
    "#     \"LS type\": names,\n",
    "#     \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "#     \"AUC\": auc_scores,\n",
    "#     \"Accuracy\": fixed_balanced_accuracies,\n",
    "#     \"Recall\": fixed_recalls,\n",
    "#     \"MCC\": fixed_mccs,\n",
    "#     \"Kappa\": fixed_kappa_scores,\n",
    "#     \"F1\": fixed_f1_scores,\n",
    "#     \"F2\": fixed_f2_scores\n",
    "# })\n",
    "\n",
    "# # Save the results as CSV files\n",
    "# # results_mcc_df.to_csv(\"Plots/results_mcc_daily_75.csv\", index=False)\n",
    "# # results_fixed_df.to_csv(\"Plots/results_fixed_daily_75.csv\", index=False)\n",
    "\n",
    "# # Print the DataFrames\n",
    "# print(\"Results using best threshold (MCC):\")\n",
    "# print(results_mcc_df)\n",
    "# print(\"\\nResults using fixed threshold (0.5):\")\n",
    "# print(results_fixed_df)\n",
    "\n",
    "\n",
    "# SAVE THE PREDICTS with time\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_daily_75_sep24.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOR only COSTANT TERRAIN\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(4, 4), dpi=500)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_terrain_Y_test.npy\",\n",
    "    \"Data/DF_terrain_Y_test.npy\",\n",
    "    \"Data/ES_terrain_Y_test.npy\",\n",
    "    \"Data/EF_terrain_Y_test.npy\",\n",
    "    \"Data/RS_terrain_Y_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_terrain_pred.npy\",\n",
    "    \"Data/DF_terrain_pred.npy\",\n",
    "    \"Data/ES_terrain_pred.npy\",\n",
    "    \"Data/EF_terrain_pred.npy\",\n",
    "    \"Data/RS_terrain_pred.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "best_thresholds = []\n",
    "max_mccs = []\n",
    "balanced_accuracies = []\n",
    "auc_scores = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate MCC for each threshold\n",
    "    mccs = []\n",
    "    for threshold in thresholds:\n",
    "        pred_binary = pred > threshold\n",
    "        mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "        mccs.append(mcc)\n",
    "    \n",
    "    # Find the threshold that maximizes MCC\n",
    "    max_mcc = max(mccs)\n",
    "    best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    pred_binary_best = pred > best_threshold\n",
    "    acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "    recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "    f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "    f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "    # Store the results for best threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "    max_mccs.append(max_mcc)\n",
    "    balanced_accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    f2_scores.append(f2)\n",
    "    kappa_scores.append(kappa)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_balanced_accuracies.append(fixed_acc)\n",
    "    fixed_recalls.append(fixed_recall)\n",
    "    fixed_mccs.append(fixed_mcc)\n",
    "    fixed_f1_scores.append(fixed_f1)\n",
    "    fixed_f2_scores.append(fixed_f2)\n",
    "    fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.axis(\"square\")\n",
    "plt.savefig(\"Plots/roc_terrain.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame to store the results using best threshold from MCC\n",
    "results_mcc_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (MCC)\": best_thresholds,\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": balanced_accuracies,\n",
    "    \"Recall\": recalls,\n",
    "    \"MCC\": max_mccs,\n",
    "    \"Kappa\": kappa_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"F2\": f2_scores\n",
    "})\n",
    "\n",
    "# Create DataFrame to store the results using fixed threshold 0.5\n",
    "results_fixed_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": fixed_balanced_accuracies,\n",
    "    \"Recall\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the results as CSV files\n",
    "#results_mcc_df.to_csv(\"Plots/results_mcc_terrain.csv\", index=False)\n",
    "#results_fixed_df.to_csv(\"Plots/results_fixed_terrain.csv\", index=False)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Results using best threshold (MCC) only terrain:\")\n",
    "print(results_mcc_df)\n",
    "print(\"\\nResults using fixed threshold (0.5) only terrain:\")\n",
    "print(results_fixed_df)\n",
    "\n",
    "# SAVE THE PREDICTS with only terrain\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_terrain.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + month SCALAR RAINFALL\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(4, 4), dpi=500)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_terrain_MonCum_Y_test.npy\",\n",
    "    \"Data/DF_terrain_MonCum_Y_test.npy\",\n",
    "    \"Data/ES_terrain_MonCum_Y_test.npy\",\n",
    "    \"Data/EF_terrain_MonCum_Y_test.npy\",\n",
    "    \"Data/RS_terrain_MonCum_Y_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_terrain_MonCum_pred.npy\",\n",
    "    \"Data/DF_terrain_MonCum_pred.npy\",\n",
    "    \"Data/ES_terrain_MonCum_pred.npy\",\n",
    "    \"Data/EF_terrain_MonCum_pred.npy\",\n",
    "    \"Data/RS_terrain_MonCum_pred.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "best_thresholds = []\n",
    "max_mccs = []\n",
    "balanced_accuracies = []\n",
    "auc_scores = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate MCC for each threshold\n",
    "    mccs = []\n",
    "    for threshold in thresholds:\n",
    "        pred_binary = pred > threshold\n",
    "        mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "        mccs.append(mcc)\n",
    "    \n",
    "    # Find the threshold that maximizes MCC\n",
    "    max_mcc = max(mccs)\n",
    "    best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    pred_binary_best = pred > best_threshold\n",
    "    acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "    recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "    f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "    f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "    # Store the results for best threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "    max_mccs.append(max_mcc)\n",
    "    balanced_accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    f2_scores.append(f2)\n",
    "    kappa_scores.append(kappa)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_balanced_accuracies.append(fixed_acc)\n",
    "    fixed_recalls.append(fixed_recall)\n",
    "    fixed_mccs.append(fixed_mcc)\n",
    "    fixed_f1_scores.append(fixed_f1)\n",
    "    fixed_f2_scores.append(fixed_f2)\n",
    "    fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.axis(\"square\")\n",
    "plt.savefig(\"Plots/roc_terrain_MonCum.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame to store the results using best threshold from MCC\n",
    "results_mcc_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (MCC)\": best_thresholds,\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": balanced_accuracies,\n",
    "    \"Recall\": recalls,\n",
    "    \"MCC\": max_mccs,\n",
    "    \"Kappa\": kappa_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"F2\": f2_scores\n",
    "})\n",
    "\n",
    "# Create DataFrame to store the results using fixed threshold 0.5\n",
    "results_fixed_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": fixed_balanced_accuracies,\n",
    "    \"Recall\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the results as CSV files\n",
    "#results_mcc_df.to_csv(\"Plots/results_mcc_terrain_MonCum.csv\", index=False)\n",
    "#results_fixed_df.to_csv(\"Plots/results_fixed_terrain_MonCum.csv\", index=False)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Results using best threshold (MCC) only terrain_MonCum:\")\n",
    "print(results_mcc_df)\n",
    "print(\"\\nResults using fixed threshold (0.5) only terrain_MonCum:\")\n",
    "print(results_fixed_df)\n",
    "\n",
    "# SAVE THE PREDICTS with only terrain\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_terrain_MonCum.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + 3 week SCALAR RAINFALL\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(4, 4), dpi=500)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_terrain_3wCum_Y_test.npy\",\n",
    "    \"Data/DF_terrain_3wCum_Y_test.npy\",\n",
    "    \"Data/ES_terrain_3wCum_Y_test.npy\",\n",
    "    \"Data/EF_terrain_3wCum_Y_test.npy\",\n",
    "    \"Data/RS_terrain_3wCum_Y_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_terrain_3wCum_pred.npy\",\n",
    "    \"Data/DF_terrain_3wCum_pred.npy\",\n",
    "    \"Data/ES_terrain_3wCum_pred.npy\",\n",
    "    \"Data/EF_terrain_3wCum_pred.npy\",\n",
    "    \"Data/RS_terrain_3wCum_pred.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "best_thresholds = []\n",
    "max_mccs = []\n",
    "balanced_accuracies = []\n",
    "auc_scores = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate MCC for each threshold\n",
    "    mccs = []\n",
    "    for threshold in thresholds:\n",
    "        pred_binary = pred > threshold\n",
    "        mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "        mccs.append(mcc)\n",
    "    \n",
    "    # Find the threshold that maximizes MCC\n",
    "    max_mcc = max(mccs)\n",
    "    best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    pred_binary_best = pred > best_threshold\n",
    "    acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "    recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "    f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "    f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "    # Store the results for best threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "    max_mccs.append(max_mcc)\n",
    "    balanced_accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    f2_scores.append(f2)\n",
    "    kappa_scores.append(kappa)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_balanced_accuracies.append(fixed_acc)\n",
    "    fixed_recalls.append(fixed_recall)\n",
    "    fixed_mccs.append(fixed_mcc)\n",
    "    fixed_f1_scores.append(fixed_f1)\n",
    "    fixed_f2_scores.append(fixed_f2)\n",
    "    fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.axis(\"square\")\n",
    "plt.savefig(\"Plots/roc_terrain_3wCum.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame to store the results using best threshold from MCC\n",
    "results_mcc_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (MCC)\": best_thresholds,\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": balanced_accuracies,\n",
    "    \"Recall\": recalls,\n",
    "    \"MCC\": max_mccs,\n",
    "    \"Kappa\": kappa_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"F2\": f2_scores\n",
    "})\n",
    "\n",
    "# Create DataFrame to store the results using fixed threshold 0.5\n",
    "results_fixed_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": fixed_balanced_accuracies,\n",
    "    \"Recall\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the results as CSV files\n",
    "#results_mcc_df.to_csv(\"Plots/results_mcc_terrain_3wCum.csv\", index=False)\n",
    "#results_fixed_df.to_csv(\"Plots/results_fixed_terrain_3wCum.csv\", index=False)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Results using best threshold (MCC) only terrain_3wCum:\")\n",
    "print(results_mcc_df)\n",
    "print(\"\\nResults using fixed threshold (0.5) only terrain_3wCum:\")\n",
    "print(results_fixed_df)\n",
    "\n",
    "# SAVE THE PREDICTS with only terrain\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_terrain_3wCum.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + 2 week SCALAR RAINFALL\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(4, 4), dpi=500)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_terrain_2wCum_Y_test.npy\",\n",
    "    \"Data/DF_terrain_2wCum_Y_test.npy\",\n",
    "    \"Data/ES_terrain_2wCum_Y_test.npy\",\n",
    "    \"Data/EF_terrain_2wCum_Y_test.npy\",\n",
    "    \"Data/RS_terrain_2wCum_Y_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_terrain_2wCum_pred.npy\",\n",
    "    \"Data/DF_terrain_2wCum_pred.npy\",\n",
    "    \"Data/ES_terrain_2wCum_pred.npy\",\n",
    "    \"Data/EF_terrain_2wCum_pred.npy\",\n",
    "    \"Data/RS_terrain_2wCum_pred.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "best_thresholds = []\n",
    "max_mccs = []\n",
    "balanced_accuracies = []\n",
    "auc_scores = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate MCC for each threshold\n",
    "    mccs = []\n",
    "    for threshold in thresholds:\n",
    "        pred_binary = pred > threshold\n",
    "        mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "        mccs.append(mcc)\n",
    "    \n",
    "    # Find the threshold that maximizes MCC\n",
    "    max_mcc = max(mccs)\n",
    "    best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    pred_binary_best = pred > best_threshold\n",
    "    acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "    recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "    f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "    f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "    # Store the results for best threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "    max_mccs.append(max_mcc)\n",
    "    balanced_accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    f2_scores.append(f2)\n",
    "    kappa_scores.append(kappa)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_balanced_accuracies.append(fixed_acc)\n",
    "    fixed_recalls.append(fixed_recall)\n",
    "    fixed_mccs.append(fixed_mcc)\n",
    "    fixed_f1_scores.append(fixed_f1)\n",
    "    fixed_f2_scores.append(fixed_f2)\n",
    "    fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.axis(\"square\")\n",
    "plt.savefig(\"Plots/roc_terrain_2wCum.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame to store the results using best threshold from MCC\n",
    "results_mcc_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (MCC)\": best_thresholds,\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": balanced_accuracies,\n",
    "    \"Recall\": recalls,\n",
    "    \"MCC\": max_mccs,\n",
    "    \"Kappa\": kappa_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"F2\": f2_scores\n",
    "})\n",
    "\n",
    "# Create DataFrame to store the results using fixed threshold 0.5\n",
    "results_fixed_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": fixed_balanced_accuracies,\n",
    "    \"Recall\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the results as CSV files\n",
    "#results_mcc_df.to_csv(\"Plots/results_mcc_terrain_2wCum.csv\", index=False)\n",
    "#results_fixed_df.to_csv(\"Plots/results_fixed_terrain_2wCum.csv\", index=False)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Results using best threshold (MCC) only terrain_2wCum:\")\n",
    "print(results_mcc_df)\n",
    "print(\"\\nResults using fixed threshold (0.5) only terrain_2wCum:\")\n",
    "print(results_fixed_df)\n",
    "\n",
    "# SAVE THE PREDICTS with only terrain\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_terrain_2wCum.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + 1 week SCALAR RAINFALL\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(4, 4), dpi=500)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_terrain_1wCum_Y_test.npy\",\n",
    "    \"Data/DF_terrain_1wCum_Y_test.npy\",\n",
    "    \"Data/ES_terrain_1wCum_Y_test.npy\",\n",
    "    \"Data/EF_terrain_1wCum_Y_test.npy\",\n",
    "    \"Data/RS_terrain_1wCum_Y_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_terrain_1wCum_pred.npy\",\n",
    "    \"Data/DF_terrain_1wCum_pred.npy\",\n",
    "    \"Data/ES_terrain_1wCum_pred.npy\",\n",
    "    \"Data/EF_terrain_1wCum_pred.npy\",\n",
    "    \"Data/RS_terrain_1wCum_pred.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "best_thresholds = []\n",
    "max_mccs = []\n",
    "balanced_accuracies = []\n",
    "auc_scores = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate MCC for each threshold\n",
    "    mccs = []\n",
    "    for threshold in thresholds:\n",
    "        pred_binary = pred > threshold\n",
    "        mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "        mccs.append(mcc)\n",
    "    \n",
    "    # Find the threshold that maximizes MCC\n",
    "    max_mcc = max(mccs)\n",
    "    best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    pred_binary_best = pred > best_threshold\n",
    "    acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "    recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "    f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "    f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "    # Store the results for best threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "    max_mccs.append(max_mcc)\n",
    "    balanced_accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    f2_scores.append(f2)\n",
    "    kappa_scores.append(kappa)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_balanced_accuracies.append(fixed_acc)\n",
    "    fixed_recalls.append(fixed_recall)\n",
    "    fixed_mccs.append(fixed_mcc)\n",
    "    fixed_f1_scores.append(fixed_f1)\n",
    "    fixed_f2_scores.append(fixed_f2)\n",
    "    fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.axis(\"square\")\n",
    "plt.savefig(\"Plots/roc_terrain_1wCum.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame to store the results using best threshold from MCC\n",
    "results_mcc_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (MCC)\": best_thresholds,\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": balanced_accuracies,\n",
    "    \"Recall\": recalls,\n",
    "    \"MCC\": max_mccs,\n",
    "    \"Kappa\": kappa_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"F2\": f2_scores\n",
    "})\n",
    "\n",
    "# Create DataFrame to store the results using fixed threshold 0.5\n",
    "results_fixed_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": fixed_balanced_accuracies,\n",
    "    \"Recall\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the results as CSV files\n",
    "#results_mcc_df.to_csv(\"Plots/results_mcc_terrain_1wCum.csv\", index=False)\n",
    "#results_fixed_df.to_csv(\"Plots/results_fixed_terrain_1wCum.csv\", index=False)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Results using best threshold (MCC) only terrain_1wCum:\")\n",
    "print(results_mcc_df)\n",
    "print(\"\\nResults using fixed threshold (0.5) only terrain_1wCum:\")\n",
    "print(results_fixed_df)\n",
    "\n",
    "# SAVE THE PREDICTS with only terrain\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_terrain_1wCum.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COSTANT TERRAIN + all weeeks and month scalar rain (cumulative)\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(4, 4), dpi=500)\n",
    "lw = 1\n",
    "\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_terrain_allCum_Y_test.npy\",\n",
    "    \"Data/DF_terrain_allCum_Y_test.npy\",\n",
    "    \"Data/ES_terrain_allCum_Y_test.npy\",\n",
    "    \"Data/EF_terrain_allCum_Y_test.npy\",\n",
    "    \"Data/RS_terrain_allCum_Y_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_terrain_allCum_pred.npy\",\n",
    "    \"Data/DF_terrain_allCum_pred.npy\",\n",
    "    \"Data/ES_terrain_allCum_pred.npy\",\n",
    "    \"Data/EF_terrain_allCum_pred.npy\",\n",
    "    \"Data/RS_terrain_allCum_pred.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"DS\",\n",
    "    \"DF\",\n",
    "    \"ES\",\n",
    "    \"EF\",\n",
    "    \"RS\",\n",
    "]\n",
    "colors = [\"blue\", \"gold\", \"black\", \"darkgrey\", \"red\"]\n",
    "\n",
    "# Lists to store the best thresholds, MCCs, and other metrics using best threshold from MCC\n",
    "best_thresholds = []\n",
    "max_mccs = []\n",
    "balanced_accuracies = []\n",
    "auc_scores = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "f2_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate MCC for each threshold\n",
    "    mccs = []\n",
    "    for threshold in thresholds:\n",
    "        pred_binary = pred > threshold\n",
    "        mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary)\n",
    "        mccs.append(mcc)\n",
    "    \n",
    "    # Find the threshold that maximizes MCC\n",
    "    max_mcc = max(mccs)\n",
    "    best_threshold = thresholds[mccs.index(max_mcc)]\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    pred_binary_best = pred > best_threshold\n",
    "    acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_best)\n",
    "    recall = sklearn.metrics.recall_score(Ydata, pred_binary_best)\n",
    "    f1 = sklearn.metrics.f1_score(Ydata, pred_binary_best)\n",
    "    f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_best, beta=2)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_best)\n",
    "    \n",
    "    # Store the results for best threshold\n",
    "    best_thresholds.append(best_threshold)\n",
    "    max_mccs.append(max_mcc)\n",
    "    balanced_accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    f2_scores.append(f2)\n",
    "    kappa_scores.append(kappa)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_balanced_accuracies.append(fixed_acc)\n",
    "    fixed_recalls.append(fixed_recall)\n",
    "    fixed_mccs.append(fixed_mcc)\n",
    "    fixed_f1_scores.append(fixed_f1)\n",
    "    fixed_f2_scores.append(fixed_f2)\n",
    "    fixed_kappa_scores.append(fixed_kappa)\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]}(auc ={round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "ax = plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.axis(\"square\")\n",
    "plt.savefig(\"Plots/roc_terrain_allCum.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame to store the results using best threshold from MCC\n",
    "results_mcc_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (MCC)\": best_thresholds,\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": balanced_accuracies,\n",
    "    \"Recall\": recalls,\n",
    "    \"MCC\": max_mccs,\n",
    "    \"Kappa\": kappa_scores,\n",
    "    \"F1\": f1_scores,\n",
    "    \"F2\": f2_scores\n",
    "})\n",
    "\n",
    "# Create DataFrame to store the results using fixed threshold 0.5\n",
    "results_fixed_df = pd.DataFrame({\n",
    "    \"LS type\": names,\n",
    "    \"Best threshold (Fixed)\": [0.5]*len(names),\n",
    "    \"AUC\": auc_scores,\n",
    "    \"Accuracy\": fixed_balanced_accuracies,\n",
    "    \"Recall\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the results as CSV files\n",
    "#results_mcc_df.to_csv(\"Plots/results_mcc_terrain_allCum.csv\", index=False)\n",
    "#results_fixed_df.to_csv(\"Plots/results_fixed_terrain_allCum.csv\", index=False)\n",
    "\n",
    "# Print the DataFrames\n",
    "print(\"Results using best threshold (MCC) only terrain_allCum:\")\n",
    "print(results_mcc_df)\n",
    "print(\"\\nResults using fixed threshold (0.5) only terrain_allCum:\")\n",
    "print(results_fixed_df)\n",
    "\n",
    "# SAVE THE PREDICTS with only terrain\n",
    "\n",
    "# Load the base GeoDataFrame from a shapefile using pyogrio\n",
    "base_shapefile_path = \"Data/su05_final.shp\"\n",
    "locations = gpd.read_file(base_shapefile_path, engine='pyogrio')\n",
    "\n",
    "# Load predictions and ensure they match the length of the GeoDataFrame\n",
    "for i in range(len(names)):\n",
    "    pred = np.load(preds[i])\n",
    "    \n",
    "    # Ensure the length of predictions matches the length of the GeoDataFrame\n",
    "    if len(pred) != len(locations):\n",
    "        raise ValueError(f\"Length of values ({len(pred)}) does not match length of index ({len(locations)})\")\n",
    "    \n",
    "    name = names[i]\n",
    "    locations[name] = pred\n",
    "\n",
    "# Save the updated GeoDataFrame to a shapefile using pyogrio\n",
    "locations.to_file(\"Results_GIS/allProbs_terrain_allCum.shp\", driver=\"ESRI Shapefile\", engine='pyogrio')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for different ls types comparing different configuration, put inference OFF to evaluate the metrics for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS # COMPARE THE RESULTS FOR EACH LS TYPE FOR ALL THE DIFFERENT PARAMS #--->> with hour data\n",
    "# (only terrain, 1,2,3, mon, and all scalar rain varaible, waveform time daily, and ...)\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "lw=0.7\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DS_terrain_Y_test_test.npy\",\n",
    "    \"Data/DS_terrain_1wCum_Y_test_test.npy\",\n",
    "    \"Data/DS_terrain_2wCum_Y_test_test.npy\",\n",
    "    \"Data/DS_terrain_3wCum_Y_test_test.npy\",\n",
    "    \"Data/DS_terrain_MonCum_Y_test_test.npy\",\n",
    "    \"Data/DS_terrain_AllCum_Y_test_test.npy\",\n",
    "    \"Data/DS_daily_75_Y_test_test.npy\",\n",
    "    \"Data/DS_hourly_all_Y_test_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DS_terrain_pred_test.npy\",\n",
    "    \"Data/DS_terrain_1wCum_pred_test.npy\",\n",
    "    \"Data/DS_terrain_2wCum_pred_test.npy\",\n",
    "    \"Data/DS_terrain_3wCum_pred_test.npy\",\n",
    "    \"Data/DS_terrain_MonCum_pred_test.npy\",\n",
    "    \"Data/DS_terrain_AllCum_pred_test.npy\",\n",
    "    \"Data/DS_daily_75_pred_test.npy\",\n",
    "    \"Data/DS_hourly_all_pred_test.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"Terrain\",\n",
    "    \"1 week\",\n",
    "    \"2 weeks\",\n",
    "    \"3 weeks\",\n",
    "    \"1 month\",\n",
    "    \"All scalar values\",\n",
    "    \"Time series (daily)\",\n",
    "    \"Time series (hourly)\",\n",
    "]\n",
    "colors = [\"black\", \"blue\", \"green\", \"gold\", \"red\", \"grey\", \"purple\", \"pink\"]\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_aucs = []\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "# Plot the ROC curve for each configuration\n",
    "plt.figure(figsize=(6, 6), dpi=600)\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_aucs.append(round(auc, 3))\n",
    "    fixed_balanced_accuracies.append(round(fixed_acc, 3))\n",
    "    fixed_recalls.append(round(fixed_recall, 3))\n",
    "    fixed_mccs.append(round(fixed_mcc, 3))\n",
    "    fixed_f1_scores.append(round(fixed_f1, 3))\n",
    "    fixed_f2_scores.append(round(fixed_f2, 3))\n",
    "    fixed_kappa_scores.append(round(fixed_kappa, 3))\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]} (AUC = {round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=10)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=10)\n",
    "plt.title(\"ROC Curves for Different Configurations for DS\", fontsize=10)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 10})\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Plots/DS_roc_comparison.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Configuration\": names,\n",
    "    \"AUC\": fixed_aucs,\n",
    "    \"Acc\": fixed_balanced_accuracies,\n",
    "    \"Rec\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the DataFrame as an Excel file\n",
    "metrics_df.to_excel(\"Plots/DS_metrics.xlsx\", index=False)\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "metrics_melted_df = metrics_df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(6, 6), dpi=500)\n",
    "sns.scatterplot(\n",
    "    data=metrics_melted_df,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Configuration\",\n",
    "    style=\"Configuration\",\n",
    "    palette=colors,\n",
    "    s=50,  # Increased size of the points for clarity\n",
    "    markers=[\"o\", \"s\", \"D\", \"^\", \"P\", \"*\", \"X\", \"X\"],  # Different marker shapes for each config\n",
    "    zorder=3  # Ensures that the symbols are on top of the grid\n",
    ")\n",
    "\n",
    "# Adjust the y-axis to start from 0 to accommodate all scores\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, zorder=0)  # Set the grid behind the plots\n",
    "plt.yticks(np.arange(0, 1, 0.05))  # Show a tick every 0.1 on the y-axis\n",
    "plt.xlabel(\"Evaluation Metric\", fontsize=10)\n",
    "plt.ylabel(\"Metric Score\", fontsize=10)\n",
    "plt.title(\"Comparison of Metrics Across Different Configurations for DS\", fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(loc=\"lower left\", prop={\"size\": 10})\n",
    "plt.savefig(\"Plots/DS_metrics_comparison.png\")\n",
    "plt.savefig(\"Plots/DS_metrics_comparison.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF # COMPARE THE RESULTS FOR EACH LS TYPE FOR ALL THE DIFFERENT PARAMS #--->> with hour data\n",
    "# (only terrain, 1,2,3, mon, and all scalar rain varaible, waveform time daily, and ...)\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "lw=0.7\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/DF_terrain_Y_test_test.npy\",\n",
    "    \"Data/DF_terrain_1wCum_Y_test_test.npy\",\n",
    "    \"Data/DF_terrain_2wCum_Y_test_test.npy\",\n",
    "    \"Data/DF_terrain_3wCum_Y_test_test.npy\",\n",
    "    \"Data/DF_terrain_MonCum_Y_test_test.npy\",\n",
    "    \"Data/DF_terrain_AllCum_Y_test_test.npy\",\n",
    "    \"Data/DF_daily_75_Y_test_test.npy\",\n",
    "    \"Data/DF_hourly_all_Y_test_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/DF_terrain_pred_test.npy\",\n",
    "    \"Data/DF_terrain_1wCum_pred_test.npy\",\n",
    "    \"Data/DF_terrain_2wCum_pred_test.npy\",\n",
    "    \"Data/DF_terrain_3wCum_pred_test.npy\",\n",
    "    \"Data/DF_terrain_MonCum_pred_test.npy\",\n",
    "    \"Data/DF_terrain_AllCum_pred_test.npy\",\n",
    "    \"Data/DF_daily_75_pred_test.npy\",\n",
    "    \"Data/DF_hourly_all_pred_test.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"Terrain\",\n",
    "    \"1 week\",\n",
    "    \"2 weeks\",\n",
    "    \"3 weeks\",\n",
    "    \"1 month\",\n",
    "    \"All scalar values\",\n",
    "    \"Time series (daily)\",\n",
    "    \"Time series (hourly)\",\n",
    "]\n",
    "colors = [\"black\", \"blue\", \"green\", \"gold\", \"red\", \"grey\", \"purple\", \"pink\"]\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_aucs = []\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "# Plot the ROC curve for each configuration\n",
    "plt.figure(figsize=(6, 6), dpi=600)\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_aucs.append(round(auc, 3))\n",
    "    fixed_balanced_accuracies.append(round(fixed_acc, 3))\n",
    "    fixed_recalls.append(round(fixed_recall, 3))\n",
    "    fixed_mccs.append(round(fixed_mcc, 3))\n",
    "    fixed_f1_scores.append(round(fixed_f1, 3))\n",
    "    fixed_f2_scores.append(round(fixed_f2, 3))\n",
    "    fixed_kappa_scores.append(round(fixed_kappa, 3))\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]} (AUC = {round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=10)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=10)\n",
    "plt.title(\"ROC Curves for Different Configurations for DF\", fontsize=10)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 10})\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Plots/DF_roc_comparison.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Configuration\": names,\n",
    "    \"AUC\": fixed_aucs,\n",
    "    \"Acc\": fixed_balanced_accuracies,\n",
    "    \"Rec\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the DataFrame as an Excel file\n",
    "metrics_df.to_excel(\"Plots/DF_metrics.xlsx\", index=False)\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "metrics_melted_df = metrics_df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(6, 6), dpi=500)\n",
    "sns.scatterplot(\n",
    "    data=metrics_melted_df,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Configuration\",\n",
    "    style=\"Configuration\",\n",
    "    palette=colors,\n",
    "    s=50,  # Increased size of the points for clarity\n",
    "    markers=[\"o\", \"s\", \"D\", \"^\", \"P\", \"*\", \"X\", \"X\"],  # Different marker shapes for each config\n",
    "    zorder=3  # Ensures that the symbols are on top of the grid\n",
    ")\n",
    "\n",
    "# Adjust the y-axis to start from 0 to accommodate all scores\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, zorder=0)  # Set the grid behind the plots\n",
    "plt.yticks(np.arange(0, 1, 0.05))  # Show a tick every 0.1 on the y-axis\n",
    "plt.xlabel(\"Evaluation Metric\", fontsize=10)\n",
    "plt.ylabel(\"Metric Score\", fontsize=10)\n",
    "plt.title(\"Comparison of Metrics Across Different Configurations for DF\", fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(loc=\"lower left\", prop={\"size\": 10})\n",
    "plt.savefig(\"Plots/DF_metrics_comparison.png\")\n",
    "plt.savefig(\"Plots/DF_metrics_comparison.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES # COMPARE THE RESULTS FOR EACH LS TYPE FOR ALL THE DIFFERENT PARAMS #--->> with hour data\n",
    "# (only terrain, 1,2,3, mon, and all scalar rain varaible, waveform time daily, and ...)\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "lw=0.7\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/ES_terrain_Y_test_test.npy\",\n",
    "    \"Data/ES_terrain_1wCum_Y_test_test.npy\",\n",
    "    \"Data/ES_terrain_2wCum_Y_test_test.npy\",\n",
    "    \"Data/ES_terrain_3wCum_Y_test_test.npy\",\n",
    "    \"Data/ES_terrain_MonCum_Y_test_test.npy\",\n",
    "    \"Data/ES_terrain_AllCum_Y_test_test.npy\",\n",
    "    \"Data/ES_daily_75_Y_test_test.npy\",\n",
    "    \"Data/ES_hourly_all_Y_test_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/ES_terrain_pred_test.npy\",\n",
    "    \"Data/ES_terrain_1wCum_pred_test.npy\",\n",
    "    \"Data/ES_terrain_2wCum_pred_test.npy\",\n",
    "    \"Data/ES_terrain_3wCum_pred_test.npy\",\n",
    "    \"Data/ES_terrain_MonCum_pred_test.npy\",\n",
    "    \"Data/ES_terrain_AllCum_pred_test.npy\",\n",
    "    \"Data/ES_daily_75_pred_test.npy\",\n",
    "    \"Data/ES_hourly_all_pred_test.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"Terrain\",\n",
    "    \"1 week\",\n",
    "    \"2 weeks\",\n",
    "    \"3 weeks\",\n",
    "    \"1 month\",\n",
    "    \"All scalar values\",\n",
    "    \"Time series (daily)\",\n",
    "    \"Time series (hourly)\",\n",
    "]\n",
    "colors = [\"black\", \"blue\", \"green\", \"gold\", \"red\", \"grey\", \"purple\", \"pink\"]\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_aucs = []\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "# Plot the ROC curve for each configuration\n",
    "plt.figure(figsize=(6, 6), dpi=600)\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_aucs.append(round(auc, 3))\n",
    "    fixed_balanced_accuracies.append(round(fixed_acc, 3))\n",
    "    fixed_recalls.append(round(fixed_recall, 3))\n",
    "    fixed_mccs.append(round(fixed_mcc, 3))\n",
    "    fixed_f1_scores.append(round(fixed_f1, 3))\n",
    "    fixed_f2_scores.append(round(fixed_f2, 3))\n",
    "    fixed_kappa_scores.append(round(fixed_kappa, 3))\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]} (AUC = {round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=10)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=10)\n",
    "plt.title(\"ROC Curves for Different Configurations for ES\", fontsize=10)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 10})\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Plots/ES_roc_comparison.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Configuration\": names,\n",
    "    \"AUC\": fixed_aucs,\n",
    "    \"Acc\": fixed_balanced_accuracies,\n",
    "    \"Rec\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the DataFrame as an Excel file\n",
    "metrics_df.to_excel(\"Plots/ES_metrics.xlsx\", index=False)\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "metrics_melted_df = metrics_df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(6, 6), dpi=500)\n",
    "sns.scatterplot(\n",
    "    data=metrics_melted_df,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Configuration\",\n",
    "    style=\"Configuration\",\n",
    "    palette=colors,\n",
    "    s=50,  # Increased size of the points for clarity\n",
    "    markers=[\"o\", \"s\", \"D\", \"^\", \"P\", \"*\", \"X\", \"X\"],  # Different marker shapes for each config\n",
    "    zorder=3  # Ensures that the symbols are on top of the grid\n",
    ")\n",
    "\n",
    "# Adjust the y-axis to start from 0 to accommodate all scores\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, zorder=0)  # Set the grid behind the plots\n",
    "plt.yticks(np.arange(0, 1, 0.05))  # Show a tick every 0.1 on the y-axis\n",
    "plt.xlabel(\"Evaluation Metric\", fontsize=10)\n",
    "plt.ylabel(\"Metric Score\", fontsize=10)\n",
    "plt.title(\"Comparison of Metrics Across Different Configurations for ES\", fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(loc=\"lower left\", prop={\"size\": 10})\n",
    "plt.savefig(\"Plots/ES_metrics_comparison.png\")\n",
    "plt.savefig(\"Plots/ES_metrics_comparison.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EF # COMPARE THE RESULTS FOR EACH LS TYPE FOR ALL THE DIFFERENT PARAMS #--->> with hour data\n",
    "# (only terrain, 1,2,3, mon, and all scalar rain varaible, waveform time daily, and ...)\n",
    "\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "lw=0.7\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/EF_terrain_Y_test_test.npy\",\n",
    "    \"Data/EF_terrain_1wCum_Y_test_test.npy\",\n",
    "    \"Data/EF_terrain_2wCum_Y_test_test.npy\",\n",
    "    \"Data/EF_terrain_3wCum_Y_test_test.npy\",\n",
    "    \"Data/EF_terrain_MonCum_Y_test_test.npy\",\n",
    "    \"Data/EF_terrain_AllCum_Y_test_test.npy\",\n",
    "    \"Data/EF_daily_75_Y_test_test.npy\",\n",
    "    \"Data/EF_hourly_all_Y_test_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/EF_terrain_pred_test.npy\",\n",
    "    \"Data/EF_terrain_1wCum_pred_test.npy\",\n",
    "    \"Data/EF_terrain_2wCum_pred_test.npy\",\n",
    "    \"Data/EF_terrain_3wCum_pred_test.npy\",\n",
    "    \"Data/EF_terrain_MonCum_pred_test.npy\",\n",
    "    \"Data/EF_terrain_AllCum_pred_test.npy\",\n",
    "    \"Data/EF_daily_75_pred_test.npy\",\n",
    "    \"Data/EF_hourly_all_pred_test.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"Terrain\",\n",
    "    \"1 week\",\n",
    "    \"2 weeks\",\n",
    "    \"3 weeks\",\n",
    "    \"1 month\",\n",
    "    \"All scalar values\",\n",
    "    \"Time series (daily)\",\n",
    "    \"Time series (hourly)\",\n",
    "]\n",
    "colors = [\"black\", \"blue\", \"green\", \"gold\", \"red\", \"grey\", \"purple\", \"pink\"]\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_aucs = []\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "# Plot the ROC curve for each configuration\n",
    "plt.figure(figsize=(6, 6), dpi=600)\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_aucs.append(round(auc, 3))\n",
    "    fixed_balanced_accuracies.append(round(fixed_acc, 3))\n",
    "    fixed_recalls.append(round(fixed_recall, 3))\n",
    "    fixed_mccs.append(round(fixed_mcc, 3))\n",
    "    fixed_f1_scores.append(round(fixed_f1, 3))\n",
    "    fixed_f2_scores.append(round(fixed_f2, 3))\n",
    "    fixed_kappa_scores.append(round(fixed_kappa, 3))\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]} (AUC = {round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=10)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=10)\n",
    "plt.title(\"ROC Curves for Different Configurations for EF\", fontsize=10)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 10})\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Plots/EF_roc_comparison.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Configuration\": names,\n",
    "    \"AUC\": fixed_aucs,\n",
    "    \"Acc\": fixed_balanced_accuracies,\n",
    "    \"Rec\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the DataFrame as an Excel file\n",
    "metrics_df.to_excel(\"Plots/EF_metrics.xlsx\", index=False)\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "metrics_melted_df = metrics_df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(6, 6), dpi=500)\n",
    "sns.scatterplot(\n",
    "    data=metrics_melted_df,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Configuration\",\n",
    "    style=\"Configuration\",\n",
    "    palette=colors,\n",
    "    s=50,  # Increased size of the points for clarity\n",
    "    markers=[\"o\", \"s\", \"D\", \"^\", \"P\", \"*\", \"X\", \"X\"],  # Different marker shapes for each config\n",
    "    zorder=3  # Ensures that the symbols are on top of the grid\n",
    ")\n",
    "\n",
    "# Adjust the y-axis to start from 0 to accommodate all scores\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, zorder=0)  # Set the grid behind the plots\n",
    "plt.yticks(np.arange(0, 1, 0.05))  # Show a tick every 0.1 on the y-axis\n",
    "plt.xlabel(\"Evaluation Metric\", fontsize=10)\n",
    "plt.ylabel(\"Metric Score\", fontsize=10)\n",
    "plt.title(\"Comparison of Metrics Across Different Configurations for EF\", fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(loc=\"lower left\", prop={\"size\": 10})\n",
    "plt.savefig(\"Plots/EF_metrics_comparison.png\")\n",
    "plt.savefig(\"Plots/EF_metrics_comparison.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RS # COMPARE THE RESULTS FOR EACH LS TYPE FOR ALL THE DIFFERENT PARAMS #--->> with hour data\n",
    "# (only terrain, 1,2,3, mon, and all scalar rain varaible, waveform time daily, and ...)\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.interpolate import interp1d\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "lw=0.7\n",
    "# Load all the data and make plot\n",
    "y_data = [\n",
    "    \"Data/RS_terrain_Y_test_test.npy\",\n",
    "    \"Data/RS_terrain_1wCum_Y_test_test.npy\",\n",
    "    \"Data/RS_terrain_2wCum_Y_test_test.npy\",\n",
    "    \"Data/RS_terrain_3wCum_Y_test_test.npy\",\n",
    "    \"Data/RS_terrain_MonCum_Y_test_test.npy\",\n",
    "    \"Data/RS_terrain_AllCum_Y_test_test.npy\",\n",
    "    \"Data/RS_daily_75_Y_test_test.npy\",\n",
    "    \"Data/RS_hourly_all_Y_test_test.npy\",\n",
    "]\n",
    "preds = [\n",
    "    \"Data/RS_terrain_pred_test.npy\",\n",
    "    \"Data/RS_terrain_1wCum_pred_test.npy\",\n",
    "    \"Data/RS_terrain_2wCum_pred_test.npy\",\n",
    "    \"Data/RS_terrain_3wCum_pred_test.npy\",\n",
    "    \"Data/RS_terrain_MonCum_pred_test.npy\",\n",
    "    \"Data/RS_terrain_AllCum_pred_test.npy\",\n",
    "    \"Data/RS_daily_75_pred_test.npy\",\n",
    "    \"Data/RS_hourly_all_pred_test.npy\",\n",
    "]\n",
    "names = [\n",
    "    \"Terrain\",\n",
    "    \"1 week\",\n",
    "    \"2 weeks\",\n",
    "    \"3 weeks\",\n",
    "    \"1 month\",\n",
    "    \"All scalar values\",\n",
    "    \"Time series (daily)\",\n",
    "    \"Time series (hourly)\",\n",
    "]\n",
    "colors = [\"black\", \"blue\", \"green\", \"gold\", \"red\", \"grey\", \"purple\", \"pink\"]\n",
    "\n",
    "# Lists to store metrics using fixed threshold 0.5\n",
    "fixed_aucs = []\n",
    "fixed_balanced_accuracies = []\n",
    "fixed_recalls = []\n",
    "fixed_mccs = []\n",
    "fixed_kappa_scores = []\n",
    "fixed_f1_scores = []\n",
    "fixed_f2_scores = []\n",
    "\n",
    "# Plot the ROC curve for each configuration\n",
    "plt.figure(figsize=(6, 6), dpi=600)\n",
    "for i in range(len(names)):\n",
    "    Ydata = np.load(y_data[i])\n",
    "    pred = np.load(preds[i])\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(Ydata, pred)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "    # unique_fpr, unique_indices = np.unique(fpr, return_index=True)\n",
    "    # unique_tpr = tpr[unique_indices]\n",
    "\n",
    "    # # Interpolation to get a smoother curve\n",
    "    # fpr_interp = np.linspace(unique_fpr.min(), unique_fpr.max(), 100000)  # points for fine resolution\n",
    "    # tpr_interp = interp1d(unique_fpr, unique_tpr, kind='linear')(fpr_interp)  # Linear interpolation\n",
    "    \n",
    "    # Calculate metrics using fixed threshold 0.5\n",
    "    pred_binary_fixed = pred > 0.5\n",
    "    fixed_acc = sklearn.metrics.balanced_accuracy_score(Ydata, pred_binary_fixed)\n",
    "    fixed_recall = sklearn.metrics.recall_score(Ydata, pred_binary_fixed)\n",
    "    fixed_mcc = sklearn.metrics.matthews_corrcoef(Ydata, pred_binary_fixed)\n",
    "    fixed_f1 = sklearn.metrics.f1_score(Ydata, pred_binary_fixed)\n",
    "    fixed_f2 = sklearn.metrics.fbeta_score(Ydata, pred_binary_fixed, beta=2)\n",
    "    fixed_kappa = sklearn.metrics.cohen_kappa_score(Ydata, pred_binary_fixed)\n",
    "    \n",
    "    # Store the results for fixed threshold\n",
    "    fixed_aucs.append(round(auc, 3))\n",
    "    fixed_balanced_accuracies.append(round(fixed_acc, 3))\n",
    "    fixed_recalls.append(round(fixed_recall, 3))\n",
    "    fixed_mccs.append(round(fixed_mcc, 3))\n",
    "    fixed_f1_scores.append(round(fixed_f1, 3))\n",
    "    fixed_f2_scores.append(round(fixed_f2, 3))\n",
    "    fixed_kappa_scores.append(round(fixed_kappa, 3))\n",
    "    \n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=colors[i],\n",
    "        lw=lw,\n",
    "        label=f\"{names[i]} (AUC = {round(auc, 2)})\",\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=10)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=10)\n",
    "plt.title(\"ROC Curves for Different Configurations for RS\", fontsize=10)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\": 10})\n",
    "plt.grid(True)\n",
    "plt.savefig(\"Plots/RS_roc_comparison.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Configuration\": names,\n",
    "    \"AUC\": fixed_aucs,\n",
    "    \"Acc\": fixed_balanced_accuracies,\n",
    "    \"Rec\": fixed_recalls,\n",
    "    \"MCC\": fixed_mccs,\n",
    "    \"Kappa\": fixed_kappa_scores,\n",
    "    \"F1\": fixed_f1_scores,\n",
    "    \"F2\": fixed_f2_scores\n",
    "})\n",
    "\n",
    "# Save the DataFrame as an Excel file\n",
    "metrics_df.to_excel(\"Plots/RS_metrics.xlsx\", index=False)\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "metrics_melted_df = metrics_df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(6, 6), dpi=500)\n",
    "sns.scatterplot(\n",
    "    data=metrics_melted_df,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Configuration\",\n",
    "    style=\"Configuration\",\n",
    "    palette=colors,\n",
    "    s=50,  # Increased size of the points for clarity\n",
    "    markers=[\"o\", \"s\", \"D\", \"^\", \"P\", \"*\", \"X\", \"X\"],  # Different marker shapes for each config\n",
    "    zorder=3  # Ensures that the symbols are on top of the grid\n",
    ")\n",
    "\n",
    "# Adjust the y-axis to start from 0 to accommodate all scores\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, zorder=0)  # Set the grid behind the plots\n",
    "plt.yticks(np.arange(0, 1, 0.05))  # Show a tick every 0.1 on the y-axis\n",
    "plt.xlabel(\"Evaluation Metric\", fontsize=10)\n",
    "plt.ylabel(\"Metric Score\", fontsize=10)\n",
    "plt.title(\"Comparison of Metrics Across Different Configurations for RS\", fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(loc=\"upper right\", prop={\"size\": 10})\n",
    "plt.savefig(\"Plots/RS_metrics_comparison.png\")\n",
    "plt.savefig(\"Plots/RS_metrics_comparison.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List of landslide types and corresponding model configurations\n",
    "landslide_types = ['DS', 'DF', 'ES', 'EF', 'RS', 'Overall']\n",
    "model_configs = [\n",
    "    'Terrain', '1 week', '2 weeks', '3 weeks', '1 month', \n",
    "    'All scalar values', 'Time series (daily)', 'Time series (hourly)'\n",
    "]\n",
    "\n",
    "# Loop through each landslide type and analyze the metrics\n",
    "for landslide in landslide_types:\n",
    "    try:\n",
    "        # Load the saved .npy file with allow_pickle=True\n",
    "        data = np.load(f'{landslide}_metrics.npy', allow_pickle=True)\n",
    "        \n",
    "        # Convert the data to a pandas DataFrame for easier analysis\n",
    "        df = pd.DataFrame(data, columns=['AUC', 'Acc', 'Rec', 'MCC', 'Kappa', 'F1', 'F2'])\n",
    "        \n",
    "        print(f'Analysis for {landslide}:')\n",
    "        print('=' * 50)\n",
    "\n",
    "        # Iterate over each column (metric)\n",
    "        for column in df.columns:\n",
    "            # Extract the values for \"Time series (daily)\" and \"Time series (hourly)\"\n",
    "            daily_value = df[column][model_configs.index('Time series (daily)')]\n",
    "            hourly_value = df[column][model_configs.index('Time series (hourly)')]\n",
    "\n",
    "            # Find the maximum value among all other configurations (excluding \"Time series (daily)\" and \"Time series (hourly)\")\n",
    "            other_values = df[column].drop([model_configs.index('Time series (daily)'), model_configs.index('Time series (hourly)')])\n",
    "            max_other_value = other_values.max()\n",
    "            max_other_idx = other_values.idxmax()\n",
    "\n",
    "            # Calculate the variation for \"Time series (daily)\" and \"Time series (hourly)\" compared to the highest of all others\n",
    "            daily_variation = ((daily_value - max_other_value) / max_other_value) * 100\n",
    "            hourly_variation = ((hourly_value - max_other_value) / max_other_value) * 100\n",
    "\n",
    "            # Print the results for this metric\n",
    "            print(f'Metric: {column}')\n",
    "            print(f'  Highest value (excluding time series): {max_other_value:.3f} (Configuration: {model_configs[max_other_idx]})')\n",
    "            print(f'  \"Time series (daily)\" value: {daily_value:.3f} - Variation: {daily_variation:.2f}%')\n",
    "            print(f'  \"Time series (hourly)\" value: {hourly_value:.3f} - Variation: {hourly_variation:.2f}%\\n')\n",
    "\n",
    "        print('=' * 50 + '\\n')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'File for {landslide} not found.')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading {landslide}_metrics.npy: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twente_old_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
